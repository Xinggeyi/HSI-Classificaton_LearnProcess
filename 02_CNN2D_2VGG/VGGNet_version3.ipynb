{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        exit(-1)"
   ]
  },
  {
   "source": [
    "# data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCHSZ = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train_label.npy\n"
     ]
    }
   ],
   "source": [
    "data_dir= \"E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3200, 9, 9, 204), (3200,), (50929, 9, 9, 204), (50929,))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train = np.load(data_root[4])\n",
    "train_label = np.load(data_root[5])\n",
    "test = np.load(data_root[2])\n",
    "test_label = np.load(data_root[3])\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tf.keras.utils.to_categorical(train_label)\n",
    "test_label = tf.keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 16\n",
    "im_height = 9\n",
    "im_width = 9\n",
    "im_channel = train.shape[3]\n",
    "train_num = train.shape[0]\n",
    "val_num = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "source": [
    "- prefetch(AUTOTUNE)\n",
    "- 当GPU执行在当前批次执行前向或者后向传播时，我们希望CPU处理下一个批次的数据，以便于数据批次能够迅速被GPU使用。我们希望GPU被完全、时刻用于训练。我们称这种机制为消费者/生产者重叠，消费者是GPU，生产者是CPU。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# dataset顺序：\n",
    "\n",
    "- 创建实例                             from_tensor_slices                       \n",
    "- 重组（较大的buffer_size）             shuffle\n",
    "- 重复                                  repeat\n",
    "- 数据预处理、数据扩增，使用多线程等                  map\n",
    "- 批次化                                batch\n",
    "- 预取数据                             prefectch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "train_db = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_num).repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "train_db= train_db.shuffle(buffer_size=train_num).batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "\n",
    "# load test dataset\n",
    "test_db = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "# val_dataset = val_dataset.repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "test_db = test_db.batch(BATCHSZ).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "train_db, test_db "
   ]
  },
  {
   "source": [
    "# model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 是多个 BasicBlock 顿叠而成\n",
    "class VGG(keras.Model):\n",
    "    \n",
    "    # layer_dims [2,2,2,2]\n",
    "    def __init__(self, num_calsses=16):   # layer_dims [2,2,2,2] 每一层的basic block个数\n",
    "        super(VGG, self).__init__()\n",
    "        self.conv_layers = Sequential([  # 5 units of conv + max pooling\n",
    "                            # unit 1\n",
    "                            layers.Conv2D(64, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.Conv2D(64, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "                            # unit 2\n",
    "                            layers.Conv2D(128, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.Conv2D(128, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "                            # unit 3\n",
    "                            layers.Conv2D(256, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.Conv2D(256, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "                            # unit 4\n",
    "                            layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "                            \n",
    "                            # unit 5\n",
    "                            layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "                            layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same')\n",
    "                            ])\n",
    "        self.flatten = layers.Flatten()\n",
    "        self.fc_net = Sequential([\n",
    "                                 layers.Dense(256, activation='relu'),\n",
    "                                 layers.Dense(128, activation='relu'),\n",
    "                                 layers.Dense(16, activation='relu'),\n",
    "                            ])\n",
    "    def call(self, inputs, training = None):\n",
    "        x = self.conv_layers(inputs)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG13():\n",
    "    return VGG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG()\n",
    "model.build(input_shape=(None, 9, 9, 204))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"vgg\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsequential (Sequential)      (None, 1, 1, 512)         9520768   \n_________________________________________________________________\nflatten (Flatten)            multiple                  0         \n_________________________________________________________________\nsequential_1 (Sequential)    (None, 16)                166288    \n=================================================================\nTotal params: 9,687,056\nTrainable params: 9,687,056\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 现在的 loss 非常小， 因为这个问题比较复杂\n",
    "optimizer = optimizers.Adam(lr=1e-4)"
   ]
  },
  {
   "source": [
    "# 训练1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]0 0 loss 2.572810649871826\n",
      "  5%|▌         | 1/20 [00:07<02:20,  7.38s/it]0 acc 0.10671719452571227\n",
      "1 0 loss 2.276338577270508\n",
      " 10%|█         | 2/20 [00:14<02:12,  7.35s/it]1 acc 0.19756916491586327\n",
      "2 0 loss 1.9665740728378296\n",
      " 15%|█▌        | 3/20 [00:21<02:04,  7.30s/it]2 acc 0.42205815939837815\n",
      "3 0 loss 1.8189773559570312\n",
      " 20%|██        | 4/20 [00:29<01:56,  7.28s/it]3 acc 0.2711421783266901\n",
      "4 0 loss 1.6107630729675293\n",
      " 25%|██▌       | 5/20 [00:36<01:48,  7.25s/it]4 acc 0.4230006479608867\n",
      "5 0 loss 1.756507158279419\n",
      " 30%|███       | 6/20 [00:43<01:41,  7.24s/it]5 acc 0.46150523277503974\n",
      "6 0 loss 1.491671085357666\n",
      " 35%|███▌      | 7/20 [00:50<01:33,  7.22s/it]6 acc 0.5392605391819985\n",
      "7 0 loss 1.5345306396484375\n",
      " 40%|████      | 8/20 [00:57<01:26,  7.23s/it]7 acc 0.47085157768658326\n",
      "8 0 loss 1.4169940948486328\n",
      " 45%|████▌     | 9/20 [01:05<01:19,  7.24s/it]8 acc 0.3544149698600012\n",
      "9 0 loss 1.4634439945220947\n",
      " 50%|█████     | 10/20 [01:12<01:12,  7.23s/it]9 acc 0.4942567103222133\n",
      "10 0 loss 1.6833171844482422\n",
      " 55%|█████▌    | 11/20 [01:19<01:05,  7.23s/it]10 acc 0.4616819493805101\n",
      "11 0 loss 1.6686607599258423\n",
      " 60%|██████    | 12/20 [01:26<00:57,  7.25s/it]11 acc 0.5207642011427673\n",
      "12 0 loss 1.4019465446472168\n",
      " 65%|██████▌   | 13/20 [01:34<00:50,  7.25s/it]12 acc 0.6078069469261128\n",
      "13 0 loss 1.5097782611846924\n",
      " 70%|███████   | 14/20 [01:41<00:43,  7.23s/it]13 acc 0.5855406546368473\n",
      "14 0 loss 1.3778724670410156\n",
      " 75%|███████▌  | 15/20 [01:48<00:36,  7.24s/it]14 acc 0.6596634530424709\n",
      "15 0 loss 1.337963342666626\n",
      " 80%|████████  | 16/20 [01:55<00:29,  7.27s/it]15 acc 0.5217655952404328\n",
      "16 0 loss 0.9712041616439819\n",
      " 85%|████████▌ | 17/20 [02:03<00:21,  7.26s/it]16 acc 0.6557560525437374\n",
      "17 0 loss 1.0880813598632812\n",
      " 90%|█████████ | 18/20 [02:10<00:14,  7.24s/it]17 acc 0.6359245223742858\n",
      "18 0 loss 0.99009108543396\n",
      " 95%|█████████▌| 19/20 [02:17<00:07,  7.21s/it]18 acc 0.6190382689626736\n",
      "19 0 loss 0.9297011494636536\n",
      "100%|██████████| 20/20 [02:24<00:00,  7.23s/it]19 acc 0.5990300221877516\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(20)):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 9, 9, 204] -> [b, 1, 1, 512]\n",
    "            logits = model(x)\n",
    "            y_onehot = tf.one_hot(y, depth=16)\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # 反向传播\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 100 == 0:\n",
    "            print(epoch, step, 'loss', float(loss))\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # 再哪里进行测试，要自己把握，测试的时间影像训练效率\n",
    "    for x, y in test_db:\n",
    "        logits = model(x)\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        # int64 -> int32\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # booler -> int\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc', acc)"
   ]
  },
  {
   "source": [
    "# 训练2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSZ = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VGG()\n",
    "model.build(input_shape=(None, 9, 9, 204))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 3.3377 - accuracy: 0.4153 - val_loss: 3.8973 - val_accuracy: 0.4769\n",
      "Epoch 2/10\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 4.9204 - accuracy: 0.2769 - val_loss: 4.7909 - val_accuracy: 0.3539\n",
      "Epoch 3/10\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 3.4018 - accuracy: 0.4219 - val_loss: 4.8942 - val_accuracy: 0.4301\n",
      "Epoch 4/10\n",
      "50/50 [==============================] - 6s 128ms/step - loss: 4.6449 - accuracy: 0.3162 - val_loss: 5.7752 - val_accuracy: 0.3578\n",
      "Epoch 5/10\n",
      "50/50 [==============================] - 6s 129ms/step - loss: 4.9715 - accuracy: 0.4266 - val_loss: 8.1723 - val_accuracy: 0.3104\n",
      "Epoch 6/10\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 7.2815 - accuracy: 0.2438 - val_loss: 8.1885 - val_accuracy: 0.2915\n",
      "Epoch 7/10\n",
      "50/50 [==============================] - 7s 130ms/step - loss: 6.2486 - accuracy: 0.2453 - val_loss: 5.8375 - val_accuracy: 0.3099\n",
      "Epoch 8/10\n",
      "50/50 [==============================] - 6s 130ms/step - loss: 4.1982 - accuracy: 0.3363 - val_loss: 5.8280 - val_accuracy: 0.1966\n",
      "Epoch 9/10\n",
      "50/50 [==============================] - 7s 132ms/step - loss: 4.0544 - accuracy: 0.3791 - val_loss: 5.6778 - val_accuracy: 0.4370\n",
      "Epoch 10/10\n",
      "50/50 [==============================] - 7s 133ms/step - loss: 4.0584 - accuracy: 0.3991 - val_loss: 5.5823 - val_accuracy: 0.3761\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
    "              # from_logits=False 如果没有进行 softmax 处理，这里执行 True\n",
    "            #   loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              #################                如果这里 from_logits=False 则训练效果会越来越差？？？                      ####################\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[\"accuracy\"])\n",
    "# change + \n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=3, min_lr=0.000001)\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./save_weights/myVGG.h5',\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,\n",
    "                                                monitor='val_loss')]\n",
    "\n",
    "# tensorflow2.1 recommend to using fit\n",
    "# history = model.fit(train_dataset,\n",
    "#                     steps_per_epoch=train_num // BATCHSZ,\n",
    "#                     epochs=EPOCHS,\n",
    "#                     validation_data=val_dataset,\n",
    "#                     validation_steps=val_num // BATCHSZ,\n",
    "#                     callbacks=[callbacks, reduce_lr])\n",
    "history = model.fit(train_db,\n",
    "                    steps_per_epoch= train_num // BATCHSZ,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=test_db,\n",
    "                    validation_steps= val_num // BATCHSZ,\n",
    "                    callbacks=[callbacks, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}