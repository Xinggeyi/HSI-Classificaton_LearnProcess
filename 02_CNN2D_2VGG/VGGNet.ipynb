{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        exit(-1)"
   ]
  },
  {
   "source": [
    "# data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCHSZ = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train_label.npy\n"
     ]
    }
   ],
   "source": [
    "data_dir= \"E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3200, 9, 9, 204), (3200,), (50929, 9, 9, 204), (50929,))"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "train = np.load(data_root[4])\n",
    "train_label = np.load(data_root[5])\n",
    "test = np.load(data_root[2])\n",
    "test_label = np.load(data_root[3])\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 16\n",
    "im_height = 9\n",
    "im_width = 9\n",
    "im_channel = train.shape[3]\n",
    "train_num = train.shape[0]\n",
    "val_num = test.shape[0]"
   ]
  },
  {
   "source": [
    "- 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "- AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "- prefetch(AUTOTUNE)\n",
    "- 当GPU执行在当前批次执行前向或者后向传播时，我们希望CPU处理下一个批次的数据，以便于数据批次能够迅速被GPU使用。我们希望GPU被完全、时刻用于训练。我们称这种机制为消费者/生产者重叠，消费者是GPU，生产者是CPU。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# dataset顺序：\n",
    "\n",
    "- 创建实例                             from_tensor_slices                       \n",
    "- 重组（较大的buffer_size）             shuffle\n",
    "- 重复                                  repeat\n",
    "- 数据预处理、数据扩增，使用多线程等                  map\n",
    "- 批次化                                batch\n",
    "- 预取数据                             prefectch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tf.keras.utils.to_categorical(train_label)\n",
    "test_label = tf.keras.utils.to_categorical(test_label)\n",
    "\n",
    "# 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# load train dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_num).repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_num).batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "\n",
    "# load test dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "# val_dataset = val_dataset.repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(BATCHSZ).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "train_dataset, val_dataset "
   ]
  },
  {
   "source": [
    "# model\n",
    "- 这个模型十分不好训练，我用2048的dense层，模型异常的大，精度几乎不会下降"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG(feature, im_height=224, im_width=224, class_num=1000, im_channel=204):\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    # change\n",
    "    # input_image = layers.Input(shape=(im_height, im_width, 3), dtype=\"float32\")\n",
    "    input_image = layers.Input(shape=(im_height, im_width, im_channel), dtype=\"float32\")\n",
    "    x = feature(input_image)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dropout(rate=0.5)(x)\n",
    "    x = layers.Dense(512, activation='relu')(x)\n",
    "    x = layers.Dense(class_num)(x)\n",
    "    output = layers.Softmax()(x)\n",
    "    model = models.Model(inputs=input_image, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(cfg):\n",
    "    feature_layers = []\n",
    "    for v in cfg:\n",
    "        if v == \"M\":\n",
    "            # change\n",
    "            # feature_layers.append(layers.MaxPool2D(pool_size=2, strides=2))\n",
    "            feature_layers.append(layers.MaxPool2D(pool_size=2, strides=1))\n",
    "        else:\n",
    "            conv2d = layers.Conv2D(v, kernel_size=3, padding=\"SAME\", activation=\"relu\")\n",
    "            feature_layers.append(conv2d)\n",
    "    return Sequential(feature_layers, name=\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络配置列表\n",
    "cfgs = {\n",
    "    'vgg11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'vgg16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'vgg19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg(model_name=\"vgg16\", im_height=224, im_width=224, class_num=1000, im_channel=204):\n",
    "    try:\n",
    "        cfg = cfgs[model_name]\n",
    "    except:\n",
    "        print(\"Warning: model number {} not in cfgs dict!\".format(model_name))\n",
    "        exit(-1)\n",
    "    model = VGG(features(cfg), im_height=im_height, im_width=im_width, class_num=class_num)\n",
    "    return model"
   ]
  },
  {
   "source": [
    "# train"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 9, 9, 204)]       0         \n_________________________________________________________________\nfeature (Sequential)         (None, 4, 4, 512)         14830464  \n_________________________________________________________________\nflatten (Flatten)            (None, 8192)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 8192)              0         \n_________________________________________________________________\ndense (Dense)                (None, 512)               4194816   \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               262656    \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                8208      \n_________________________________________________________________\nsoftmax (Softmax)            (None, 16)                0         \n=================================================================\nTotal params: 19,296,144\nTrainable params: 19,296,144\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = vgg(model_name=\"vgg16\", im_height=im_height, im_width=im_width, class_num=class_num, im_channel=204)\n",
    "model.summary()"
   ]
  },
  {
   "source": [
    "# using keras low level api for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create direction for saving weights\n",
    "if not os.path.exists(\"save_weights_lowAPI\"):\n",
    "    os.makedirs(\"save_weights_lowAPI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # dropout 层需要设置 trianing \n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    # dropout 层需要设置 trianing \n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.144110599999998\n",
      "Epoch 1, Loss: 2.535879611968994, Accuracy: 11.03125, Test Loss: 2.379885196685791, Test Accuracy: 11.79289722442627\n",
      "2.8634209000000084\n",
      "Epoch 2, Loss: 2.2259793281555176, Accuracy: 12.5625, Test Loss: 2.1895194053649902, Test Accuracy: 8.699323654174805\n",
      "2.921755399999995\n",
      "Epoch 3, Loss: 2.1155753135681152, Accuracy: 12.15625, Test Loss: 2.0884039402008057, Test Accuracy: 9.113764762878418\n",
      "2.9216214000000065\n",
      "Epoch 4, Loss: 1.8438879251480103, Accuracy: 21.875, Test Loss: 1.5178767442703247, Test Accuracy: 36.21346664428711\n",
      "2.8975075999999973\n",
      "Epoch 5, Loss: 1.6513177156448364, Accuracy: 27.90625, Test Loss: 1.4209561347961426, Test Accuracy: 30.82770347595215\n",
      "2.8979975000000024\n",
      "Epoch 6, Loss: 1.5108940601348877, Accuracy: 34.34375, Test Loss: 1.473039150238037, Test Accuracy: 23.744892120361328\n",
      "2.921158900000023\n",
      "Epoch 7, Loss: 1.409835934638977, Accuracy: 40.75, Test Loss: 1.1576513051986694, Test Accuracy: 51.327781677246094\n",
      "2.9019185999999877\n",
      "Epoch 8, Loss: 1.2806270122528076, Accuracy: 46.5, Test Loss: 1.1752102375030518, Test Accuracy: 56.60944747924805\n",
      "2.918151600000016\n",
      "Epoch 9, Loss: 1.1602555513381958, Accuracy: 51.84375, Test Loss: 1.5476794242858887, Test Accuracy: 28.726037979125977\n",
      "2.985522400000008\n",
      "Epoch 10, Loss: 1.0452288389205933, Accuracy: 55.843753814697266, Test Loss: 0.8483210206031799, Test Accuracy: 59.08430099487305\n",
      "2.9498020999999994\n",
      "Epoch 11, Loss: 0.9542731642723083, Accuracy: 58.40625, Test Loss: 0.9355060458183289, Test Accuracy: 58.988059997558594\n",
      "2.965512600000011\n",
      "Epoch 12, Loss: 0.9016396403312683, Accuracy: 60.531253814697266, Test Loss: 0.8113190531730652, Test Accuracy: 53.215354919433594\n",
      "3.0372516999999846\n",
      "Epoch 13, Loss: 0.8369097709655762, Accuracy: 62.6875, Test Loss: 1.0052438974380493, Test Accuracy: 55.20505905151367\n",
      "2.9619379000000094\n",
      "Epoch 14, Loss: 0.8580436706542969, Accuracy: 63.75, Test Loss: 0.7591249942779541, Test Accuracy: 68.40430450439453\n",
      "2.971310599999981\n",
      "Epoch 15, Loss: 0.8699952960014343, Accuracy: 62.8125, Test Loss: 0.8063564300537109, Test Accuracy: 56.93549346923828\n",
      "2.9803997999999865\n",
      "Epoch 16, Loss: 0.729938805103302, Accuracy: 68.03125, Test Loss: 0.8547344207763672, Test Accuracy: 60.10371017456055\n",
      "2.9796082999999953\n",
      "Epoch 17, Loss: 0.7710058689117432, Accuracy: 65.4375, Test Loss: 0.8056573867797852, Test Accuracy: 55.57236099243164\n",
      "2.969957000000022\n",
      "Epoch 18, Loss: 0.7514612674713135, Accuracy: 65.46875, Test Loss: 0.7740780711174011, Test Accuracy: 62.68463134765625\n",
      "2.9952774000000204\n",
      "Epoch 19, Loss: 0.7864106893539429, Accuracy: 66.5, Test Loss: 0.7029427886009216, Test Accuracy: 64.76665496826172\n",
      "2.96900949999997\n",
      "Epoch 20, Loss: 0.6699262857437134, Accuracy: 69.96875, Test Loss: 0.7119622230529785, Test Accuracy: 64.800048828125\n",
      "2.9625302000000033\n",
      "Epoch 21, Loss: 0.6742461919784546, Accuracy: 69.59375, Test Loss: 0.722192108631134, Test Accuracy: 68.56340026855469\n",
      "2.982760799999994\n",
      "Epoch 22, Loss: 0.6307646036148071, Accuracy: 71.96875, Test Loss: 0.7184032797813416, Test Accuracy: 65.16342163085938\n",
      "2.9776378999999906\n",
      "Epoch 23, Loss: 0.5828937292098999, Accuracy: 72.9375, Test Loss: 0.6542777419090271, Test Accuracy: 64.63702392578125\n",
      "2.9634598000000096\n",
      "Epoch 24, Loss: 0.6719330549240112, Accuracy: 70.28125, Test Loss: 0.8686334490776062, Test Accuracy: 59.02537536621094\n",
      "2.9772077999999738\n",
      "Epoch 25, Loss: 0.7184551954269409, Accuracy: 68.6875, Test Loss: 0.6945889592170715, Test Accuracy: 68.7755355834961\n",
      "2.9675697999999784\n",
      "Epoch 26, Loss: 0.6386122703552246, Accuracy: 70.78125, Test Loss: 0.8150703310966492, Test Accuracy: 53.89888381958008\n",
      "2.972481899999991\n",
      "Epoch 27, Loss: 0.6235145330429077, Accuracy: 70.8125, Test Loss: 0.5634153485298157, Test Accuracy: 69.91278839111328\n",
      "3.020383200000026\n",
      "Epoch 28, Loss: 0.5425037145614624, Accuracy: 75.40625, Test Loss: 0.6425142288208008, Test Accuracy: 64.02812957763672\n",
      "3.0393934000000513\n",
      "Epoch 29, Loss: 0.5254393219947815, Accuracy: 75.9375, Test Loss: 0.5593404769897461, Test Accuracy: 76.35527801513672\n",
      "3.009686600000009\n",
      "Epoch 30, Loss: 0.559457004070282, Accuracy: 73.21875, Test Loss: 0.6644232869148254, Test Accuracy: 71.71393585205078\n",
      "2.989884400000051\n",
      "Epoch 31, Loss: 0.5049853324890137, Accuracy: 75.625, Test Loss: 0.6719209551811218, Test Accuracy: 62.737667083740234\n",
      "2.9677381000000196\n",
      "Epoch 32, Loss: 0.49465182423591614, Accuracy: 76.15625, Test Loss: 0.6044086217880249, Test Accuracy: 67.708984375\n",
      "2.8790559999999914\n",
      "Epoch 33, Loss: 0.5676848888397217, Accuracy: 73.875, Test Loss: 0.7026572227478027, Test Accuracy: 63.18156814575195\n",
      "2.9782672999999704\n",
      "Epoch 34, Loss: 0.6208537817001343, Accuracy: 73.15625, Test Loss: 0.6273263692855835, Test Accuracy: 72.09302520751953\n",
      "2.9985719000000017\n",
      "Epoch 35, Loss: 0.53937166929245, Accuracy: 74.53125, Test Loss: 0.6483559608459473, Test Accuracy: 73.03582763671875\n",
      "2.9732334000000264\n",
      "Epoch 36, Loss: 0.49406901001930237, Accuracy: 77.125, Test Loss: 0.5384168028831482, Test Accuracy: 76.0194091796875\n",
      "2.9567815000000337\n",
      "Epoch 37, Loss: 0.5554805994033813, Accuracy: 75.21875, Test Loss: 0.585591733455658, Test Accuracy: 66.9665298461914\n",
      "2.986238700000058\n",
      "Epoch 38, Loss: 0.46983572840690613, Accuracy: 77.28125, Test Loss: 0.6602981686592102, Test Accuracy: 60.54368591308594\n",
      "3.0107835000000023\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-688532b30542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mval_step_num\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    734\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# For Python 3 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    770\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 772\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    753\u001b[0m         \u001b[1;31m# and instead mimic ops placement in graphs: Operations on resource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    754\u001b[0m         \u001b[1;31m# handles execute on the same device as where the resource is placed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 755\u001b[1;33m         ret = gen_dataset_ops.iterator_get_next(\n\u001b[0m\u001b[0;32m    756\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Miniconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   2602\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2603\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2604\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   2605\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNext\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2606\u001b[0m         \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "train_step_num = train_num // BATCHSZ\n",
    "val_step_num = val_num // BATCHSZ\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    train_loss.reset_states()        # clear history info\n",
    "    train_accuracy.reset_states()    # clear history info\n",
    "    test_loss.reset_states()         # clear history info\n",
    "    test_accuracy.reset_states()     # clear history info\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    for index, (images, labels) in enumerate(train_dataset):\n",
    "        train_step(images, labels)\n",
    "        if index+1 == train_step_num:\n",
    "            break\n",
    "    print(time.perf_counter()-t1)\n",
    "\n",
    "    for index, (images, labels) in enumerate(val_dataset):\n",
    "        test_step(images, labels)\n",
    "        if index+1 == val_step_num:\n",
    "            break\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result() * 100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result() * 100))\n",
    "    if test_loss.result() < best_test_loss:\n",
    "        model.save_weights(\"./save_weights_lowAPI/myVGG.ckpt\".format(epoch), save_format='tf')"
   ]
  },
  {
   "source": [
    "# using keras high level api for training"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 16\n",
    "im_height = 9\n",
    "im_width = 9\n",
    "im_channel = train.shape[3]\n",
    "train_num = train.shape[0]\n",
    "test_num = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tf.keras.utils.to_categorical(train_label)\n",
    "test_label = tf.keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# load train dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=train_num).repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_num).batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "\n",
    "# load test dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "test_dataset = test_dataset.repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "# val_dataset = val_dataset.batch(BATCHSZ).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "train_dataset, test_dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_9\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_5 (InputLayer)         [(None, 9, 9, 204)]       0         \n_________________________________________________________________\nfeature (Sequential)         (None, 4, 4, 512)         9520768   \n_________________________________________________________________\nflatten_4 (Flatten)          (None, 8192)              0         \n_________________________________________________________________\ndropout_8 (Dropout)          (None, 8192)              0         \n_________________________________________________________________\ndense_12 (Dense)             (None, 512)               4194816   \n_________________________________________________________________\ndropout_9 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_13 (Dense)             (None, 512)               262656    \n_________________________________________________________________\ndense_14 (Dense)             (None, 16)                8208      \n_________________________________________________________________\nsoftmax_4 (Softmax)          (None, 16)                0         \n=================================================================\nTotal params: 13,986,448\nTrainable params: 13,986,448\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "model = vgg(model_name=\"vgg13\", im_height=im_height, im_width=im_width, class_num=class_num)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create direction for saving weights\n",
    "if not os.path.exists(\"save_weights\"):\n",
    "    os.makedirs(\"save_weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "  1/100 [..............................] - ETA: 0s - loss: 2.7713 - accuracy: 0.0938WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0080s vs `on_train_batch_end` time: 0.0140s). Check your callbacks.\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 2.5479 - accuracy: 0.0981 - val_loss: 2.2749 - val_accuracy: 0.1872\n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 2.1789 - accuracy: 0.1428 - val_loss: 1.9563 - val_accuracy: 0.1731\n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 1.7430 - accuracy: 0.2603 - val_loss: 1.5079 - val_accuracy: 0.3320\n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 1.5949 - accuracy: 0.3181 - val_loss: 1.3352 - val_accuracy: 0.3172\n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 1.5073 - accuracy: 0.3616 - val_loss: 1.3665 - val_accuracy: 0.4306\n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 1.2991 - accuracy: 0.4434 - val_loss: 1.1897 - val_accuracy: 0.4758\n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 1.2235 - accuracy: 0.4716 - val_loss: 1.0568 - val_accuracy: 0.4583\n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 1.0785 - accuracy: 0.5266 - val_loss: 0.8794 - val_accuracy: 0.6237\n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.9714 - accuracy: 0.5747 - val_loss: 0.8351 - val_accuracy: 0.6415\n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.8370 - accuracy: 0.6294 - val_loss: 0.8018 - val_accuracy: 0.6453\n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 13s 129ms/step - loss: 0.7919 - accuracy: 0.6500 - val_loss: 0.8735 - val_accuracy: 0.6058\n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.8622 - accuracy: 0.6203 - val_loss: 0.7188 - val_accuracy: 0.6952\n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.7247 - accuracy: 0.6728 - val_loss: 0.7067 - val_accuracy: 0.7127\n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.6581 - accuracy: 0.6956 - val_loss: 0.6098 - val_accuracy: 0.7563\n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.6277 - accuracy: 0.7203 - val_loss: 0.6519 - val_accuracy: 0.7262\n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.5760 - accuracy: 0.7394 - val_loss: 0.5721 - val_accuracy: 0.7246\n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.5562 - accuracy: 0.7487 - val_loss: 0.5737 - val_accuracy: 0.7071\n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.4963 - accuracy: 0.7756 - val_loss: 0.6618 - val_accuracy: 0.6327\n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.4696 - accuracy: 0.7941 - val_loss: 0.5448 - val_accuracy: 0.7048\n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 13s 131ms/step - loss: 0.4179 - accuracy: 0.8269 - val_loss: 0.5254 - val_accuracy: 0.7495\n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 13s 131ms/step - loss: 0.4626 - accuracy: 0.8203 - val_loss: 0.4235 - val_accuracy: 0.8331\n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.3543 - accuracy: 0.8628 - val_loss: 0.6385 - val_accuracy: 0.6847\n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.3954 - accuracy: 0.8472 - val_loss: 0.4564 - val_accuracy: 0.7792\n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.3062 - accuracy: 0.8656 - val_loss: 0.3744 - val_accuracy: 0.8580\n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.3086 - accuracy: 0.8822 - val_loss: 0.5079 - val_accuracy: 0.8008\n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2838 - accuracy: 0.8878 - val_loss: 0.4429 - val_accuracy: 0.7806\n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.3231 - accuracy: 0.8650 - val_loss: 0.5299 - val_accuracy: 0.7110\n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2648 - accuracy: 0.8831 - val_loss: 0.3650 - val_accuracy: 0.7935\n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.2272 - accuracy: 0.9059 - val_loss: 0.3613 - val_accuracy: 0.8620\n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.2439 - accuracy: 0.9013 - val_loss: 0.5456 - val_accuracy: 0.7209\n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.2701 - accuracy: 0.8991 - val_loss: 0.5508 - val_accuracy: 0.7717\n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.3028 - accuracy: 0.8847 - val_loss: 0.3398 - val_accuracy: 0.8502\n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.2017 - accuracy: 0.9172 - val_loss: 0.3231 - val_accuracy: 0.8239\n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.2740 - accuracy: 0.8938 - val_loss: 0.4086 - val_accuracy: 0.8626\n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.2248 - accuracy: 0.9103 - val_loss: 0.2677 - val_accuracy: 0.8966\n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.2037 - accuracy: 0.9141 - val_loss: 0.2577 - val_accuracy: 0.9015\n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.1586 - accuracy: 0.9375 - val_loss: 0.4943 - val_accuracy: 0.7907\n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.1849 - accuracy: 0.9222 - val_loss: 0.2652 - val_accuracy: 0.8827\n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2105 - accuracy: 0.9162 - val_loss: 0.4687 - val_accuracy: 0.7690\n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.1828 - accuracy: 0.9281 - val_loss: 0.3089 - val_accuracy: 0.8473\n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.1507 - accuracy: 0.9359 - val_loss: 0.2421 - val_accuracy: 0.8935\n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.1839 - accuracy: 0.9284 - val_loss: 0.2981 - val_accuracy: 0.8706\n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 13s 130ms/step - loss: 0.2131 - accuracy: 0.9234 - val_loss: 0.3768 - val_accuracy: 0.8519\n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2048 - accuracy: 0.9175 - val_loss: 0.3385 - val_accuracy: 0.8282\n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.1283 - accuracy: 0.9481 - val_loss: 0.2133 - val_accuracy: 0.9054\n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2217 - accuracy: 0.9225 - val_loss: 0.3578 - val_accuracy: 0.8228\n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.1222 - accuracy: 0.9494 - val_loss: 0.2478 - val_accuracy: 0.9027\n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.2082 - accuracy: 0.9194 - val_loss: 0.2265 - val_accuracy: 0.8954\n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.1005 - accuracy: 0.9566 - val_loss: 0.3955 - val_accuracy: 0.8004\n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.1155 - accuracy: 0.9559 - val_loss: 0.2723 - val_accuracy: 0.8631\n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.2369 - accuracy: 0.9109 - val_loss: 0.2779 - val_accuracy: 0.8879\n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.1226 - accuracy: 0.9509 - val_loss: 0.3148 - val_accuracy: 0.8347\n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.1050 - accuracy: 0.9578 - val_loss: 0.2347 - val_accuracy: 0.8883\n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.2325 - accuracy: 0.9134 - val_loss: 0.2334 - val_accuracy: 0.9095\n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.1205 - accuracy: 0.9538 - val_loss: 0.2377 - val_accuracy: 0.9064\n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0841 - accuracy: 0.9641 - val_loss: 0.2058 - val_accuracy: 0.9088\n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0937 - accuracy: 0.9572 - val_loss: 0.3139 - val_accuracy: 0.8223\n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.0882 - accuracy: 0.9634 - val_loss: 0.1962 - val_accuracy: 0.9104\n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.0757 - accuracy: 0.9681 - val_loss: 0.3712 - val_accuracy: 0.8631\n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.1372 - accuracy: 0.9497 - val_loss: 0.2054 - val_accuracy: 0.9184\n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.0868 - accuracy: 0.9603 - val_loss: 0.3435 - val_accuracy: 0.8599\n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.1115 - accuracy: 0.9566 - val_loss: 0.2157 - val_accuracy: 0.9080\n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.0775 - accuracy: 0.9675 - val_loss: 0.2040 - val_accuracy: 0.9080\n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0978 - accuracy: 0.9569 - val_loss: 0.1861 - val_accuracy: 0.9244\n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.1112 - accuracy: 0.9547 - val_loss: 0.1890 - val_accuracy: 0.9247\n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.0742 - accuracy: 0.9697 - val_loss: 0.1808 - val_accuracy: 0.9237\n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.0736 - accuracy: 0.9659 - val_loss: 0.1685 - val_accuracy: 0.9315\n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.0704 - accuracy: 0.9697 - val_loss: 0.3875 - val_accuracy: 0.8634\n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.1613 - accuracy: 0.9472 - val_loss: 0.1896 - val_accuracy: 0.9193\n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.0939 - accuracy: 0.9616 - val_loss: 0.4567 - val_accuracy: 0.8243\n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0755 - accuracy: 0.9653 - val_loss: 0.2514 - val_accuracy: 0.8857\n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0783 - accuracy: 0.9694 - val_loss: 0.2237 - val_accuracy: 0.9121\n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0894 - accuracy: 0.9619 - val_loss: 0.1843 - val_accuracy: 0.9326\n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.0650 - accuracy: 0.9728 - val_loss: 0.2114 - val_accuracy: 0.9127\n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.1095 - accuracy: 0.9600 - val_loss: 0.1719 - val_accuracy: 0.9298\n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.1257 - accuracy: 0.9553 - val_loss: 0.8904 - val_accuracy: 0.7729\n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0955 - accuracy: 0.9581 - val_loss: 0.2102 - val_accuracy: 0.9123\n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0658 - accuracy: 0.9700 - val_loss: 0.1699 - val_accuracy: 0.9323\n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0576 - accuracy: 0.9762 - val_loss: 0.2174 - val_accuracy: 0.9274\n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 12s 123ms/step - loss: 0.0730 - accuracy: 0.9678 - val_loss: 0.1782 - val_accuracy: 0.9294\n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.0549 - accuracy: 0.9750 - val_loss: 0.1575 - val_accuracy: 0.9340\n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.0599 - accuracy: 0.9706 - val_loss: 0.2237 - val_accuracy: 0.8975\n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.0601 - accuracy: 0.9712 - val_loss: 0.1761 - val_accuracy: 0.9348\n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0551 - accuracy: 0.9753 - val_loss: 0.3675 - val_accuracy: 0.8640\n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0735 - accuracy: 0.9712 - val_loss: 0.1697 - val_accuracy: 0.9300\n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 13s 126ms/step - loss: 0.0528 - accuracy: 0.9778 - val_loss: 0.1421 - val_accuracy: 0.9448\n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0757 - accuracy: 0.9647 - val_loss: 0.1687 - val_accuracy: 0.9228\n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0644 - accuracy: 0.9744 - val_loss: 0.1562 - val_accuracy: 0.9407\n",
      "Epoch 89/100\n",
      "100/100 [==============================] - 13s 125ms/step - loss: 0.0519 - accuracy: 0.9778 - val_loss: 0.1532 - val_accuracy: 0.9439\n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0592 - accuracy: 0.9753 - val_loss: 0.2193 - val_accuracy: 0.9084\n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.1047 - accuracy: 0.9625 - val_loss: 0.1567 - val_accuracy: 0.9369\n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0543 - accuracy: 0.9791 - val_loss: 0.1459 - val_accuracy: 0.9427\n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.0643 - accuracy: 0.9716 - val_loss: 0.1559 - val_accuracy: 0.9406\n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.0463 - accuracy: 0.9812 - val_loss: 0.1637 - val_accuracy: 0.9378\n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 12s 125ms/step - loss: 0.1347 - accuracy: 0.9572 - val_loss: 0.1703 - val_accuracy: 0.9364\n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.0501 - accuracy: 0.9803 - val_loss: 0.1536 - val_accuracy: 0.9389\n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 12s 124ms/step - loss: 0.0417 - accuracy: 0.9841 - val_loss: 0.1460 - val_accuracy: 0.9435\n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.0366 - accuracy: 0.9850 - val_loss: 0.1478 - val_accuracy: 0.9422\n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 13s 127ms/step - loss: 0.0337 - accuracy: 0.9862 - val_loss: 0.1630 - val_accuracy: 0.9385\n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 13s 128ms/step - loss: 0.0494 - accuracy: 0.9791 - val_loss: 0.1917 - val_accuracy: 0.9224\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005),\n",
    "            #   from_logits=False 如果没有进行 softmax 处理，这里执行 True\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),\n",
    "              metrics=[\"accuracy\"])\n",
    "# change + \n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=10, min_lr=0.000001)\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./save_weights/myVGG.h5',\n",
    "                                                save_best_only=True,\n",
    "                                                save_weights_only=True,\n",
    "                                                monitor='val_loss')]\n",
    "\n",
    "# tensorflow2.1 recommend to using fit\n",
    "# history = model.fit(train_dataset,\n",
    "#                     steps_per_epoch=train_num // BATCHSZ,\n",
    "#                     epochs=EPOCHS,\n",
    "#                     validation_data=val_dataset,\n",
    "#                     validation_steps=val_num // BATCHSZ,\n",
    "#                     callbacks=[callbacks, reduce_lr])\n",
    "history = model.fit(train_dataset,\n",
    "                    steps_per_epoch=train_num // BATCHSZ,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=test_dataset,\n",
    "                    validation_steps=test_num // BATCHSZ,\n",
    "                    callbacks=[callbacks, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}