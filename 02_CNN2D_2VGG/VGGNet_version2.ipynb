{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分成两部分写VGG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "        exit(-1)"
   ]
  },
  {
   "source": [
    "# data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCHSZ = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train_label.npy\n"
     ]
    }
   ],
   "source": [
    "data_dir= \"E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3200, 9, 9, 204), (3200,), (50929, 9, 9, 204), (50929,))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train = np.load(data_root[4])\n",
    "train_label = np.load(data_root[5])\n",
    "test = np.load(data_root[2])\n",
    "test_label = np.load(data_root[3])\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label = tf.keras.utils.to_categorical(train_label)\n",
    "# test_label = tf.keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 16\n",
    "im_height = 9\n",
    "im_width = 9\n",
    "im_channel = train.shape[3]\n",
    "train_num = train.shape[0]\n",
    "val_num = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "source": [
    "- prefetch(AUTOTUNE)\n",
    "- 当GPU执行在当前批次执行前向或者后向传播时，我们希望CPU处理下一个批次的数据，以便于数据批次能够迅速被GPU使用。我们希望GPU被完全、时刻用于训练。我们称这种机制为消费者/生产者重叠，消费者是GPU，生产者是CPU。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# dataset顺序：\n",
    "\n",
    "- 创建实例                             from_tensor_slices                       \n",
    "- 重组（较大的buffer_size）             shuffle\n",
    "- 重复                                  repeat\n",
    "- 数据预处理、数据扩增，使用多线程等                  map\n",
    "- 批次化                                batch\n",
    "- 预取数据                             prefectch"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "train_db = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_num).repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "train_db= train_db.shuffle(buffer_size=train_num).batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "\n",
    "# load test dataset\n",
    "test_db = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "# val_dataset = val_dataset.repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "test_db = test_db.batch(BATCHSZ).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>,\n",
       " <PrefetchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "train_db, test_db "
   ]
  },
  {
   "source": [
    "# model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layers = [  # 5 units of conv + max pooling\n",
    "    # unit 1\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.Conv2D(64, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 2\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.Conv2D(128, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 3\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.Conv2D(256, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "\n",
    "    # unit 4\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same'),\n",
    "    \n",
    "    # unit 5\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.Conv2D(512, kernel_size=[3, 3], padding='same', activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=[2, 2], strides=2, padding='same')\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 一个网络分成两部分来写，也可以部分成两部分，用 flatten 层，这里写成两部分，加深对编写网络层的理解\n",
    "# 第一部分，一个Sequential\n",
    "conv_net = Sequential(conv_layers)\n",
    "# x = tf.random.normal([4, 32, 32, 204])\n",
    "# out = conv_net(x)\n",
    "# # 从这里可以看到展平后的维度\n",
    "# print(out.shape)  # (4, 1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第一部分，另一个Sequential\n",
    "fc_net = Sequential([\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(16, activation='relu'),\n",
    "])\n",
    "\n",
    "# 网络定义完成\n",
    "conv_net.build(input_shape=[None, 9, 9, 204])\n",
    "fc_net.build(input_shape=[None, 512])\n",
    "\n",
    "# 现在的 loss 非常小， 因为这个问题比较复杂\n",
    "optimizer = optimizers.Adam(lr=1e-4)"
   ]
  },
  {
   "source": [
    "# 训练"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 两个结构的变量组合， 同时求导\n",
    "# [1, 2] + [3, 4] = [1, 2, 3, 4]\n",
    "variables = conv_net.trainable_variables + fc_net.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "  5%|▌         | 1/20 [00:07<02:19,  7.33s/it]0 acc 0.07045102004751713\n",
      " 10%|█         | 2/20 [00:14<02:11,  7.31s/it]1 acc 0.153782717115985\n",
      " 15%|█▌        | 3/20 [00:21<02:04,  7.31s/it]2 acc 0.1703940780301989\n",
      " 20%|██        | 4/20 [00:29<01:56,  7.30s/it]3 acc 0.11252920732784857\n",
      " 25%|██▌       | 5/20 [00:36<01:49,  7.31s/it]4 acc 0.40454358027842685\n",
      " 30%|███       | 6/20 [00:43<01:42,  7.33s/it]5 acc 0.22203459718431542\n",
      " 35%|███▌      | 7/20 [00:51<01:35,  7.36s/it]6 acc 0.30387402069547803\n",
      " 40%|████      | 8/20 [00:58<01:27,  7.33s/it]7 acc 0.4843213100591019\n",
      " 45%|████▌     | 9/20 [01:05<01:20,  7.33s/it]8 acc 0.473875395157965\n",
      " 50%|█████     | 10/20 [01:13<01:13,  7.37s/it]9 acc 0.5604861670168273\n",
      " 55%|█████▌    | 11/20 [01:20<01:06,  7.41s/it]10 acc 0.5149521883406311\n",
      " 60%|██████    | 12/20 [01:28<00:59,  7.41s/it]11 acc 0.5678297237330401\n",
      " 65%|██████▌   | 13/20 [01:35<00:51,  7.38s/it]12 acc 0.42476781401559033\n",
      " 70%|███████   | 14/20 [01:43<00:44,  7.39s/it]13 acc 0.43947456262640144\n",
      " 75%|███████▌  | 15/20 [01:50<00:37,  7.41s/it]14 acc 0.5323685915686543\n",
      " 80%|████████  | 16/20 [01:57<00:29,  7.44s/it]15 acc 0.5545170727876063\n",
      " 85%|████████▌ | 17/20 [02:05<00:22,  7.40s/it]16 acc 0.6095348426240452\n",
      " 90%|█████████ | 18/20 [02:12<00:14,  7.43s/it]17 acc 0.595417149364802\n",
      " 95%|█████████▌| 19/20 [02:20<00:07,  7.40s/it]18 acc 0.4337410905378075\n",
      "100%|██████████| 20/20 [02:27<00:00,  7.38s/it]19 acc 0.6436411474798248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(20)):\n",
    "    for step, (x, y) in enumerate(train_db):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 9, 9, 204] -> [b, 1, 1, 512]\n",
    "            out = conv_net(x)\n",
    "            out = tf.reshape(out, [-1, 512])\n",
    "            # [b, 512] -> [b, 16]\n",
    "            logits = fc_net(out)\n",
    "            y_onehot = tf.one_hot(y, depth=16)\n",
    "            loss = tf.losses.categorical_crossentropy(y_onehot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        # 反向传播\n",
    "        grads = tape.gradient(loss, variables)\n",
    "        optimizer.apply_gradients(zip(grads, variables))\n",
    "\n",
    "    if step % 100 == 0:\n",
    "        print(epoch, step, 'loss', float(loss))\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # 再哪里进行测试，要自己把握，测试的时间影像训练效率\n",
    "    for x, y in test_db:\n",
    "        out = conv_net(x)\n",
    "        out = tf.reshape(out, [-1, 512])\n",
    "        logtis = fc_net(out)\n",
    "        prob = tf.nn.softmax(logtis, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "\n",
    "        # int64 -> int32\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "        # booler -> int\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "\n",
    "        total_num += x.shape[0]\n",
    "        total_correct += int(correct)\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}