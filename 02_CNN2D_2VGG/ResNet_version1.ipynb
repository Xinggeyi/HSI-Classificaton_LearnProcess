{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 此代码来源于CNN1D， CNN1D 来源于github源码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib,random\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def max_min(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataSet salinas_corrected shape is (512, 217, 204)\n"
     ]
    }
   ],
   "source": [
    "# 定义所有常数    \n",
    "# 这里完成获取数据操作\n",
    "BATCHSZ = 32\n",
    "train_num = 200\n",
    "seed = 666\n",
    "data_name = r'Salinas_corrected'\n",
    "data_gt_name = r'Salinas'\n",
    "result = 'result'\n",
    "fix_seed = False\n",
    "num_calsses = 16 \n",
    "cube_size = 9\n",
    "data_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Salinas_corrected.mat\")\n",
    "data_gt_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Salinas_gt.mat\")\n",
    "\n",
    "# startswith 检查字符串是否以 \"————\" 开头, 取出数据集\n",
    "data_name = [t for t in list(data_dict.keys()) if not t.startswith('__')][0]\n",
    "data_gt_name = [t for t in list(data_gt_dict.keys()) if not t.startswith('__')][0]\n",
    "\n",
    "data = data_dict[data_name]\n",
    "\n",
    "# 标准化\n",
    "data = max_min(data).astype(np.float32)\n",
    "data_gt = data_gt_dict[data_gt_name].astype(np.int64)\n",
    "\n",
    "dim = data.shape[2]\n",
    "print('DataSet %s shape is %s'%(data_name,data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给出 row，col，返回 w_size 大小的cube???\n",
    "# row, col 为像素值的位置索引\n",
    "\n",
    "# 此处为1D CNN， 所定义的w_size 为 1\n",
    "def neighbor_add(row, col, w_size=3):  \n",
    "    t = w_size // 2\n",
    "    # 初始化立方体 shape = 1, 1, 204\n",
    "    cube = np.zeros(shape=[w_size, w_size, data.shape[2]])\n",
    "    for i in range(-t, t + 1):\n",
    "        for j in range(-t, t + 1):\n",
    "            # 如果创建的 cube 在图像之外\n",
    "            if i + row < 0 or i + row >= data.shape[0] or j + col < 0 or j + col >= data.shape[1]:\n",
    "                cube[i + t, j + t] = data[row, col]\n",
    "            else:\n",
    "                cube[i + t, j + t] = data[i + row, j + col]\n",
    "    return cube"
   ]
  },
  {
   "source": [
    "# 得到全部数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这个是未分类版本\n",
    "# class_num = np.max(data_gt)\n",
    "# data_pos = {i: [] for i in range(1, 2)}\n",
    "# print(data_pos)\n",
    "\n",
    "# for i in range(data_gt.shape[0]):\n",
    "#     for j in range(data_gt.shape[1]):\n",
    "#         if data_gt[i, j]:\n",
    "#             data_pos[1].append([i, j])\n",
    "# data_t = 0\n",
    "# data_pos_all = list()\n",
    "\n",
    "# for k,v in data_pos.items():\n",
    "#     print('data-ID %s: %s'%(k,len(v)))\n",
    "#     data_t += len(v)\n",
    "#     for t in v:\n",
    "#         data_pos_all.append([k,t])\n",
    "# print('total data %s'%data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建一个空的ndarray 用于装数据\n",
    "# data_all = np.zeros((54129, cube_size, cube_size, 204))\n",
    "# data_label_all = np.zeros((54129)).astype(\"int\")\n",
    "\n",
    "# k = 0\n",
    "# for i in data_pos_all:\n",
    "#     # print(i)\n",
    "#     # 取出训练集中的一个数，随着 i 的改变， 所取的数也会发生改变\n",
    "#     [r,c] = i[1]\n",
    "#     # print(i[1])\n",
    "#     # print(r, c)\n",
    "#     # pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32).tostring()\n",
    "#     pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32)\n",
    "#     data_all[k] = pixel_t\n",
    "#     # print(pixel_t.shape)\n",
    "#     # print(train[1000])\n",
    "    \n",
    "#     # 标签值 - 1\n",
    "#     label_t = np.array(np.array(i[0] - 1).astype(np.int64))\n",
    "#     data_label_all[k] = label_t\n",
    "#     k = k+ 1\n",
    "\n",
    "# data_all.shape, data_label_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in data_pos.items():\n",
    "#     print(k, end=\",\")\n",
    "\n",
    "# print()\n",
    "# for k, v in data_pos.items():\n",
    "#     print(len(v), end=\",\")"
   ]
  },
  {
   "source": [
    "# 划分训练集和测试集"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: []}\n"
     ]
    }
   ],
   "source": [
    "# 得到类别数 16类，从1开始\n",
    "# DataSet salinas_corrected shape is (512, 217, 204)\n",
    "# 在这里已经把标签为 0 的背景给删除了\n",
    "\n",
    "class_num = np.max(data_gt)    \n",
    "\n",
    "data_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "train_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "test_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "\n",
    "print(data_pos)\n",
    "\n",
    "for i in range(data_gt.shape[0]):\n",
    "    for j in range(data_gt.shape[1]):\n",
    "        for k in range(1, class_num + 1):\n",
    "            if data_gt[i, j] == k:\n",
    "                data_pos[k].append([i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否用随机种子\n",
    "if fix_seed:\n",
    "    random.seed(seed)\n",
    "\n",
    "# 划分训练集和测试集, 一共 3200 个\n",
    "for k, v in data_pos.items():\n",
    "    if len(v)<train_num:\n",
    "        train_num = 15\n",
    "    else:\n",
    "        train_num = train_num\n",
    "    train_pos[k] = random.sample(v, int(train_num))\n",
    "    test_pos[k] = [i for i in v if i not in train_pos[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_all = list()\n",
    "test_pos_all = list()\n",
    "for k,v in train_pos.items():\n",
    "    for t in v:\n",
    "        train_pos_all.append([k,t])\n",
    "for k,v in test_pos.items():\n",
    "    for t in v:\n",
    "        test_pos_all.append([k,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "traindata-ID 1: 200; testdata-ID 1: 1809\ntraindata-ID 2: 200; testdata-ID 2: 3526\ntraindata-ID 3: 200; testdata-ID 3: 1776\ntraindata-ID 4: 200; testdata-ID 4: 1194\ntraindata-ID 5: 200; testdata-ID 5: 2478\ntraindata-ID 6: 200; testdata-ID 6: 3759\ntraindata-ID 7: 200; testdata-ID 7: 3379\ntraindata-ID 8: 200; testdata-ID 8: 11071\ntraindata-ID 9: 200; testdata-ID 9: 6003\ntraindata-ID 10: 200; testdata-ID 10: 3078\ntraindata-ID 11: 200; testdata-ID 11: 868\ntraindata-ID 12: 200; testdata-ID 12: 1727\ntraindata-ID 13: 200; testdata-ID 13: 716\ntraindata-ID 14: 200; testdata-ID 14: 870\ntraindata-ID 15: 200; testdata-ID 15: 7068\ntraindata-ID 16: 200; testdata-ID 16: 1607\ntotal train 3200, total test 50929\n"
     ]
    }
   ],
   "source": [
    "train_t = 0\n",
    "test_t = 0\n",
    "for (k1,v1),(k2,v2) in zip(train_pos.items(), test_pos.items()):\n",
    "    print('traindata-ID %s: %s; testdata-ID %s: %s'%(k1,len(v1),k2,len(v2)))\n",
    "    train_t += len(v1)\n",
    "    test_t += len(v2)\n",
    "print('total train %s, total test %s'%(train_t,test_t))\n",
    "# for k,v in self.test_pos.items():\n",
    "#     print('testdata-ID %s: %s'%(k,len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空的ndarray 用于装数据\n",
    "train = np.zeros((3200, cube_size, cube_size, 204)).astype(np.float32)\n",
    "train_label = np.zeros((3200)).astype(np.int32)\n",
    "\n",
    "test = np.zeros((50929, cube_size, cube_size, 204)).astype(np.float32)\n",
    "test_label = np.zeros((50929)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "k = 0\n",
    "for i in train_pos_all:\n",
    "    # print(i)\n",
    "    # 取出训练集中的一个数，随着 i 的改变， 所取的数也会发生改变\n",
    "    [r,c] = i[1]\n",
    "    # print(i[1])\n",
    "    # print(r, c)\n",
    "    # pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32).tostring()\n",
    "    pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32)\n",
    "    train[k] = pixel_t\n",
    "    # print(pixel_t.shape)\n",
    "    # print(train[1000])\n",
    "    \n",
    "    # 标签值 - 1\n",
    "    label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "    train_label[k] = label_t\n",
    "    k = k+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "k = 0\n",
    "for i in test_pos_all:\n",
    "    # 取出测试集中的一个数，随着 i 的改变， 所取的数也会发生改变\n",
    "    [r, c] = i[1]\n",
    "    # pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32).tostring()\n",
    "    pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32)\n",
    "    test[k] = pixel_t\n",
    "\n",
    "    label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "    test_label[k] = label_t\n",
    "    # print('.', end='')\n",
    "    k = k+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3200, 9, 9, 204), (50929, 9, 9, 204), (3200,), (50929, 9, 9, 204))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "train.shape, test.shape,train_label.shape, test.shape"
   ]
  },
  {
   "source": [
    "# 创建dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "\n",
    "# db_train = db_train.shuffle(3800).batch(batch_size=BATCHSZ).repeat()\n",
    "db_train = db_train.shuffle(3800).batch(batch_size=BATCHSZ)    ###############\n",
    "db_test = db_test.batch(batch_size=BATCHSZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>,\n",
       " <BatchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "db_train, db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # padding 对于能够整除的四周均匀补全，对于不能整除的，自适应补全\n",
    "        # 第一层\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3,3), strides=stride, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "\n",
    "        # 第二层\n",
    "        self.conv2 = layers.Conv2D(filter_num, (3,3), strides=1, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # 保证短接后的 channels 一致\n",
    "        if stride != 1:\n",
    "            # 短接层， identity layer\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n",
    "        else:\n",
    "            self.downsample = lambda x:x\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 前向传播\n",
    "        # [b, h ,w, c]\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        identity = self.downsample(inputs)\n",
    "\n",
    "        output = layers.add([out, identity])\n",
    "        output = self.relu(output)              # output = tf.nn.relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 是多个 BasicBlock 顿叠而成\n",
    "class ResNet(keras.Model):\n",
    "\n",
    "    def __init__(self, layer_dims, num_calsses=16):   #  layer_dims [2,2,2,2] 每一层的basic block\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # 设置预处理层\n",
    "        self.stem = Sequential([layers.Conv2D(64, (3,3), strides=(1, 1)),\n",
    "                               layers.BatchNormalization(),\n",
    "                               layers.Activation('relu'),\n",
    "                               layers.MaxPooling2D(pool_size=(2,2), strides=(1, 1), padding='same')\n",
    "                               ])\n",
    "\n",
    "        self.layers1 = self.build_resblock(64, layer_dims[0])\n",
    "        # h, w 维会变小\n",
    "        self.layers2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layers3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layers4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "\n",
    "        # 全连接层 output : [b, 512, h, w]，自适应输出用于输出\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(16)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 前向运算过程，预处理\n",
    "        x = self.stem(inputs)\n",
    "        # 4 个 resBlock\n",
    "        x = self.layers1(x)\n",
    "        x = self.layers2(x)\n",
    "        x = self.layers3(x)\n",
    "        x = self.layers4(x)\n",
    "\n",
    "        # [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # [b, 100]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 实现 resblock\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "        res_blocks = Sequential()\n",
    "        # 添加第一层， may down sample\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        # 后面的 BasicBlock 不会下采样  ？？？# TODO？？？\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    return ResNet([2, 2, 2, 2])   #   1 + 4 * 4 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 9, 9, 204) (32,) tf.Tensor(0.0006509004, shape=(), dtype=float32) tf.Tensor(0.90160555, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(db_train))\n",
    "print(sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = resnet18()\n",
    "    model.build(input_shape=(None, 9, 9, 204))  # TODO 需要修改\n",
    "    model.summary()\n",
    "    optimizer = optimizers.Adam(lr=1e-3)\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        for step, (x, y) in enumerate(db_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3]  =>  [b, 100]\n",
    "                logits = model(x)\n",
    "                # print(logits.shape)\n",
    "                # [b] => [b, 100]\n",
    "                y_onthot = tf.one_hot(y, depth=num_calsses)         # TODO 需要修改\n",
    "                # print(y_onthot.shape) \n",
    "                loss = tf.losses.categorical_crossentropy(y_onthot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if step % 300 ==0:\n",
    "                print(epoch, step, 'loss', float(loss))\n",
    "\n",
    "        total_num = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for x, y in db_test:\n",
    "            logits = model(x)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num += x.shape[0]\n",
    "            \n",
    "            total_correct += int(correct)\n",
    "\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, 'acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"res_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 7, 7, 64)          117824    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 7, 7, 64)          148736    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 4, 4, 128)         526976    \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 2, 2, 256)         2102528   \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 1, 1, 512)         8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  8208      \n",
      "=================================================================\n",
      "Total params: 11,303,632\n",
      "Trainable params: 11,295,824\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n",
      "0 0 loss 2.7831785678863525\n",
      "0 acc 0.2554536707965992\n",
      "1 0 loss 1.574958324432373\n",
      "1 acc 0.3556912564550649\n",
      "2 0 loss 1.1787464618682861\n",
      "2 acc 0.5153841622651142\n",
      "3 0 loss 0.8544679880142212\n",
      "3 acc 0.5579924993618567\n",
      "4 0 loss 1.090301275253296\n",
      "4 acc 0.5967327063166369\n",
      "5 0 loss 0.6642802953720093\n",
      "5 acc 0.7025859529933829\n",
      "6 0 loss 0.557488739490509\n",
      "6 acc 0.5775687722122955\n",
      "7 0 loss 0.49995559453964233\n",
      "7 acc 0.6978931453592256\n",
      "8 0 loss 0.7131794691085815\n",
      "8 acc 0.761432582615013\n",
      "9 0 loss 0.3612869381904602\n",
      "9 acc 0.7702684128885311\n",
      "10 0 loss 0.5264178514480591\n",
      "10 acc 0.7414439710184767\n",
      "11 0 loss 0.35175153613090515\n",
      "11 acc 0.7967759037090852\n",
      "12 0 loss 0.13385644555091858\n",
      "12 acc 0.7062970017082605\n",
      "13 0 loss 0.4020461142063141\n",
      "13 acc 0.8698776728386577\n",
      "14 0 loss 0.11139428615570068\n",
      "14 acc 0.8366353158318444\n",
      "15 0 loss 0.22949272394180298\n",
      "15 acc 0.8555636277955585\n",
      "16 0 loss 0.09350456297397614\n",
      "16 acc 0.7679318266606452\n",
      "17 0 loss 0.3326859176158905\n",
      "17 acc 0.7876259105813976\n",
      "18 0 loss 0.019479557871818542\n",
      "18 acc 0.8844666103791553\n",
      "19 0 loss 0.08509181439876556\n",
      "19 acc 0.8537964617408549\n",
      "20 0 loss 0.06316596269607544\n",
      "20 acc 0.8782226236525359\n",
      "21 0 loss 0.10521036386489868\n",
      "21 acc 0.8720179072826877\n",
      "22 0 loss 0.2467576563358307\n",
      "22 acc 0.8794596398908284\n",
      "23 0 loss 0.11646123975515366\n",
      "23 acc 0.8652241355612715\n",
      "24 0 loss 0.21606239676475525\n",
      "24 acc 0.7886076695006774\n",
      "25 0 loss 0.3418287932872772\n",
      "25 acc 0.9012546878988396\n",
      "26 0 loss 0.1221027672290802\n",
      "26 acc 0.794066249091873\n",
      "27 0 loss 0.32206642627716064\n",
      "27 acc 0.8181782481493844\n",
      "28 0 loss 0.06289546936750412\n",
      "28 acc 0.8755718745704805\n",
      "29 0 loss 0.0671868622303009\n",
      "29 acc 0.8672465589349879\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "orgin shape (None, 9, 9, 128)\nafte Conv2D (None, 9, 9, 256)\nafte Conv2D (None, 9, 9, 256)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), input_shape=(cube_size,cube_size,204), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "print(\"orgin shape\", model.output.shape)\n",
    "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "print(\"afte Conv2D\", model.output_shape)\n",
    "model.add(tf.keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "# model.add(tf.keras.layers.MaxPooling2D()      # 不能添加\n",
    "print(\"afte Conv2D\", model.output_shape)\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(16, activation='softmax', name='Salinas_Output'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=10, min_lr=0.000001, verbose=1)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "              optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 10\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 2.6380 - acc: 0.5278 - val_loss: 138.5585 - val_acc: 0.0738\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 1.0763 - acc: 0.6603 - val_loss: 12.4960 - val_acc: 0.1129\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.8128 - acc: 0.7094 - val_loss: 0.9013 - val_acc: 0.5992\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.6761 - acc: 0.7303 - val_loss: 1.3447 - val_acc: 0.5757\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.5318 - acc: 0.7881 - val_loss: 2.2973 - val_acc: 0.4565\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.4856 - acc: 0.8109 - val_loss: 1.0396 - val_acc: 0.6746\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.4740 - acc: 0.8131 - val_loss: 0.3757 - val_acc: 0.8453\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 8s 79ms/step - loss: 0.4493 - acc: 0.8228 - val_loss: 0.4361 - val_acc: 0.8199\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.4130 - acc: 0.8313 - val_loss: 1.0587 - val_acc: 0.5995\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 8s 78ms/step - loss: 0.3833 - acc: 0.8534 - val_loss: 4.2389 - val_acc: 0.3656\n"
     ]
    }
   ],
   "source": [
    "# Start to train model \n",
    "history = model.fit(db_train,\n",
    "                    batch_size=batch_size, \n",
    "                    steps_per_epoch=train.shape[0]//batch_size,\n",
    "                    epochs=epoch, \n",
    "                    validation_data=db_test, \n",
    "                    validation_steps=test.shape[0]//batch_size,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}