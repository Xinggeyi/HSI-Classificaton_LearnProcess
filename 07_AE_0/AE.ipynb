{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "\n",
    "tf.random.set_seed(22)\n",
    "np.random.seed(22)\n",
    "\n",
    "# 保存图片， 将 100 张图片放到一张图片上显示\n",
    "def save_images(imgs, name):\n",
    "    new_im = Image.new('L', (280, 280))\n",
    "\n",
    "    index = 0\n",
    "    for i in range(0, 280, 28):\n",
    "        for j in range(0, 280, 28):\n",
    "            im = imgs[index]\n",
    "            im = Image.fromarray(im, mode='L')\n",
    "            new_im.paste(im, (i ,j))\n",
    "            index += 1\n",
    "\n",
    "    new_im.save(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_dim = 20\n",
    "batchsz = 512\n",
    "lr = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((60000, 28, 28), (60000,), (10000, 28, 28), (10000,))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "x_train, x_test = x_train.astype(np.float32) / 255., x_test.astype(np.float32) /255\n",
    "\n",
    "# we do not need label\n",
    "train_db = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_db = train_db.shuffle(batchsz * 5).batch(batch_size=batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices(x_test)\n",
    "test_db = test_db.batch(batch_size=batchsz)\n",
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"ae\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nsequential (Sequential)      (None, 20)                236436    \n_________________________________________________________________\nsequential_1 (Sequential)    (None, 784)               237200    \n=================================================================\nTotal params: 473,636\nTrainable params: 473,636\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class AE(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(AE, self).__init__()  # 调用父类的初始化函数\n",
    "\n",
    "        # Encoders\n",
    "        self.encoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(h_dim, activation=tf.nn.relu),\n",
    "        ])\n",
    "\n",
    "        # Decoders\n",
    "        self.decoder = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "            tf.keras.layers.Dense(784),\n",
    "        ])\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # [b, 784] => [b, 10]\n",
    "        h = self.encoder(inputs)\n",
    "        # [b, 10] => [b, 784]\n",
    "        x_hat = self.decoder(h)\n",
    "\n",
    "        return x_hat\n",
    "\n",
    "model = AE()\n",
    "model.build(input_shape=(None, 784))\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0 0 0.6928378343582153\n",
      "0 100 0.331125408411026\n",
      "1 0 0.3161986470222473\n",
      "1 100 0.30843275785446167\n",
      "2 0 0.2983492612838745\n",
      "2 100 0.30463555455207825\n",
      "3 0 0.3003339469432831\n",
      "3 100 0.29873165488243103\n",
      "4 0 0.295669823884964\n",
      "4 100 0.29646652936935425\n",
      "5 0 0.28497421741485596\n",
      "5 100 0.2913709580898285\n",
      "6 0 0.2867877185344696\n",
      "6 100 0.2954622209072113\n",
      "7 0 0.28468888998031616\n",
      "7 100 0.28305166959762573\n",
      "8 0 0.29208239912986755\n",
      "8 100 0.2935892641544342\n",
      "9 0 0.2821807563304901\n",
      "9 100 0.28092968463897705\n",
      "10 0 0.2901744544506073\n",
      "10 100 0.28266751766204834\n",
      "11 0 0.2775445878505707\n",
      "11 100 0.2835020124912262\n",
      "12 0 0.2774159908294678\n",
      "12 100 0.28485819697380066\n",
      "13 0 0.2766736149787903\n",
      "13 100 0.28215473890304565\n",
      "14 0 0.27722251415252686\n",
      "14 100 0.2868023216724396\n",
      "15 0 0.28230297565460205\n",
      "15 100 0.2785570025444031\n",
      "16 0 0.27122610807418823\n",
      "16 100 0.2805258631706238\n",
      "17 0 0.27583640813827515\n",
      "17 100 0.2775891125202179\n",
      "18 0 0.27586328983306885\n",
      "18 100 0.28493499755859375\n",
      "19 0 0.2769325077533722\n",
      "19 100 0.2782527804374695\n",
      "20 0 0.28137311339378357\n",
      "20 100 0.28162291646003723\n",
      "21 0 0.2729610800743103\n",
      "21 100 0.2871595323085785\n",
      "22 0 0.27523747086524963\n",
      "22 100 0.27615395188331604\n",
      "23 0 0.27372708916664124\n",
      "23 100 0.2812635004520416\n",
      "24 0 0.2800455093383789\n",
      "24 100 0.2774381637573242\n",
      "25 0 0.2727566063404083\n",
      "25 100 0.2791432738304138\n",
      "26 0 0.2763046622276306\n",
      "26 100 0.27870193123817444\n",
      "27 0 0.2753955125808716\n",
      "27 100 0.2751033902168274\n",
      "28 0 0.2748733162879944\n",
      "28 100 0.2720903754234314\n",
      "29 0 0.27020758390426636\n",
      "29 100 0.269778311252594\n",
      "30 0 0.27119746804237366\n",
      "30 100 0.27707812190055847\n",
      "31 0 0.2712724208831787\n",
      "31 100 0.2764818072319031\n",
      "32 0 0.2660416066646576\n",
      "32 100 0.28097283840179443\n",
      "33 0 0.2705892324447632\n",
      "33 100 0.2801763117313385\n",
      "34 0 0.2750059962272644\n",
      "34 100 0.27890074253082275\n",
      "35 0 0.2731935977935791\n",
      "35 100 0.2723716199398041\n",
      "36 0 0.27132612466812134\n",
      "36 100 0.2810470759868622\n",
      "37 0 0.2731921970844269\n",
      "37 100 0.27127987146377563\n",
      "38 0 0.2744028568267822\n",
      "38 100 0.2685105800628662\n",
      "39 0 0.2695174515247345\n",
      "39 100 0.270641565322876\n",
      "40 0 0.27424731850624084\n",
      "40 100 0.2778998017311096\n",
      "41 0 0.278660386800766\n",
      "41 100 0.2696186900138855\n",
      "42 0 0.2733300030231476\n",
      "42 100 0.2777406871318817\n",
      "43 0 0.26890379190444946\n",
      "43 100 0.2803179919719696\n",
      "44 0 0.27235591411590576\n",
      "44 100 0.2731928825378418\n",
      "45 0 0.2698513865470886\n",
      "45 100 0.2765846848487854\n",
      "46 0 0.27152538299560547\n",
      "46 100 0.2767893075942993\n",
      "47 0 0.2739643454551697\n",
      "47 100 0.2766050696372986\n",
      "48 0 0.2675386071205139\n",
      "48 100 0.27532416582107544\n",
      "49 0 0.26928266882896423\n",
      "49 100 0.27202802896499634\n",
      "50 0 0.2709251642227173\n",
      "50 100 0.2760903537273407\n",
      "51 0 0.27592387795448303\n",
      "51 100 0.2726122736930847\n",
      "52 0 0.2732486128807068\n",
      "52 100 0.2700790762901306\n",
      "53 0 0.26469019055366516\n",
      "53 100 0.2685970664024353\n",
      "54 0 0.2669046223163605\n",
      "54 100 0.2689792811870575\n",
      "55 0 0.26869258284568787\n",
      "55 100 0.26983505487442017\n",
      "56 0 0.2638910412788391\n",
      "56 100 0.2694438695907593\n",
      "57 0 0.2708874046802521\n",
      "57 100 0.27588507533073425\n",
      "58 0 0.2708582878112793\n",
      "58 100 0.2705008089542389\n",
      "59 0 0.27448832988739014\n",
      "59 100 0.2743462324142456\n",
      "60 0 0.27019014954566956\n",
      "60 100 0.273078978061676\n",
      "61 0 0.2655268907546997\n",
      "61 100 0.27477556467056274\n",
      "62 0 0.27178317308425903\n",
      "62 100 0.2778848111629486\n",
      "63 0 0.2718266546726227\n",
      "63 100 0.2708216905593872\n",
      "64 0 0.27210479974746704\n",
      "64 100 0.2649460732936859\n",
      "65 0 0.2660703659057617\n",
      "65 100 0.26671701669692993\n",
      "66 0 0.26876360177993774\n",
      "66 100 0.2774417996406555\n",
      "67 0 0.2654227912425995\n",
      "67 100 0.2782251834869385\n",
      "68 0 0.2690463960170746\n",
      "68 100 0.2693752944469452\n",
      "69 0 0.2759595513343811\n",
      "69 100 0.2761381268501282\n",
      "70 0 0.2696402668952942\n",
      "70 100 0.2672622501850128\n",
      "71 0 0.26961958408355713\n",
      "71 100 0.27563297748565674\n",
      "72 0 0.26069939136505127\n",
      "72 100 0.27710556983947754\n",
      "73 0 0.27065467834472656\n",
      "73 100 0.26897668838500977\n",
      "74 0 0.27243292331695557\n",
      "74 100 0.2796362340450287\n",
      "75 0 0.26695531606674194\n",
      "75 100 0.2716953754425049\n",
      "76 0 0.26849594712257385\n",
      "76 100 0.27292385697364807\n",
      "77 0 0.2684609293937683\n",
      "77 100 0.2715515196323395\n",
      "78 0 0.26177549362182617\n",
      "78 100 0.2738651931285858\n",
      "79 0 0.26672419905662537\n",
      "79 100 0.27566370368003845\n",
      "80 0 0.27300819754600525\n",
      "80 100 0.27594995498657227\n",
      "81 0 0.2614191174507141\n",
      "81 100 0.27619311213493347\n",
      "82 0 0.27172696590423584\n",
      "82 100 0.2721843123435974\n",
      "83 0 0.26995792984962463\n",
      "83 100 0.27138468623161316\n",
      "84 0 0.26730233430862427\n",
      "84 100 0.26817208528518677\n",
      "85 0 0.2684226930141449\n",
      "85 100 0.2683658003807068\n",
      "86 0 0.27605268359184265\n",
      "86 100 0.2787320613861084\n",
      "87 0 0.2692223787307739\n",
      "87 100 0.26999199390411377\n",
      "88 0 0.26443225145339966\n",
      "88 100 0.2722797989845276\n",
      "89 0 0.26677796244621277\n",
      "89 100 0.27689987421035767\n",
      "90 0 0.27005693316459656\n",
      "90 100 0.27737948298454285\n",
      "91 0 0.26732420921325684\n",
      "91 100 0.27269941568374634\n",
      "92 0 0.2780035138130188\n",
      "92 100 0.2747795283794403\n",
      "93 0 0.2662756145000458\n",
      "93 100 0.27639251947402954\n",
      "94 0 0.27197110652923584\n",
      "94 100 0.27283555269241333\n",
      "95 0 0.2658766210079193\n",
      "95 100 0.26320844888687134\n",
      "96 0 0.26794055104255676\n",
      "96 100 0.2725515067577362\n",
      "97 0 0.2665218710899353\n",
      "97 100 0.2745021879673004\n",
      "98 0 0.26508307456970215\n",
      "98 100 0.2801412343978882\n",
      "99 0 0.2709740698337555\n",
      "99 100 0.27525651454925537\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100):\n",
    "    for step, x in enumerate(train_db):\n",
    "\n",
    "        # [b, 28, 28] => [b, 784]\n",
    "        x = tf.reshape(x, [-1, 784])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            x_rec_logists = model(x)\n",
    "\n",
    "            rec_loss = tf.losses.binary_crossentropy(x, x_rec_logists, from_logits=True)\n",
    "            rec_loss = tf.reduce_mean(rec_loss)\n",
    "\n",
    "\n",
    "        grads = tape.gradient(rec_loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 100 ==0:\n",
    "            print(epoch, step, float(rec_loss))\n",
    "\n",
    "        # evalutation and save\n",
    "        x = next(iter(test_db))\n",
    "        logits = model(tf.reshape(x, [-1, 784]))\n",
    "        x_hat = tf.sigmoid(logits)\n",
    "\n",
    "        # x [b, 784] => [b, 28, 28]\n",
    "        x_hat = tf.reshape(x_hat, [-1, 28, 28])\n",
    "\n",
    "        # [b, 28, 28] =>  [2b, 28, 28]\n",
    "        x_concat = tf.concat([x, x_hat], axis=0)\n",
    "        x_concat = x_concat.numpy() * 255.\n",
    "        x_concat = x_concat.astype(np.uint8)\n",
    "        save_images(x_concat, 'ae_images/rec_epoch_%d.png'%epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印原始图片\n",
    "x = next(iter(test_db))\n",
    "logits = model(tf.reshape(x, [-1, 784]))\n",
    "x_hat = tf.sigmoid(logits)\n",
    "\n",
    "# x [b, 784] => [b, 28, 28]\n",
    "x_hat = tf.reshape(x_hat, [-1, 28, 28])\n",
    "\n",
    "# [b, 28, 28] =>  [2b, 28, 28]\n",
    "x_concat = tf.concat([x, x_hat], axis=0)\n",
    "x_concat = x\n",
    "x_concat = x_concat.numpy() * 255.\n",
    "x_concat = x_concat.astype(np.uint8)\n",
    "save_images(x_concat, 'ae_images/ori.png')"
   ]
  }
 ]
}