{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 此代码来源于CNN1D， CNN1D 来源于github源码"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib,random\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def max_min(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义所有常数    \n",
    "# 这里完成获取数据操作\n",
    "BATCHSZ = 32\n",
    "train_num = 200\n",
    "seed = 666\n",
    "data_name = r'Salinas_corrected'\n",
    "data_gt_name = r'Salinas'\n",
    "result = 'result'\n",
    "fix_seed = False\n",
    "num_calsses = 16 \n",
    "cube_size = 9\n",
    "data_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Salinas_corrected.mat\")\n",
    "data_gt_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Salinas_gt.mat\")\n",
    "\n",
    "# startswith 检查字符串是否以 \"————\" 开头, 取出数据集\n",
    "data_name = [t for t in list(data_dict.keys()) if not t.startswith('__')][0]\n",
    "data_gt_name = [t for t in list(data_gt_dict.keys()) if not t.startswith('__')][0]\n",
    "\n",
    "data = data_dict[data_name]\n",
    "\n",
    "# 标准化\n",
    "data = max_min(data).astype(np.float32)\n",
    "data_gt = data_gt_dict[data_gt_name].astype(np.int64)\n",
    "\n",
    "dim = data.shape[2]\n",
    "print('DataSet %s shape is %s'%(data_name,data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给出 row，col，返回 w_size 大小的cube???\n",
    "# row, col 为像素值的位置索引\n",
    "\n",
    "# 此处为1D CNN， 所定义的w_size 为 1\n",
    "def neighbor_add(row, col, w_size=3):  \n",
    "    t = w_size // 2\n",
    "    # 初始化立方体 shape = 1, 1, 204\n",
    "    cube = np.zeros(shape=[w_size, w_size, data.shape[2]])\n",
    "    for i in range(-t, t + 1):\n",
    "        for j in range(-t, t + 1):\n",
    "            # 如果创建的 cube 在图像之外\n",
    "            if i + row < 0 or i + row >= data.shape[0] or j + col < 0 or j + col >= data.shape[1]:\n",
    "                cube[i + t, j + t] = data[row, col]\n",
    "            else:\n",
    "                cube[i + t, j + t] = data[i + row, j + col]\n",
    "    return cube"
   ]
  },
  {
   "source": [
    "# 得到全部数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 这个是未分类版本\n",
    "# class_num = np.max(data_gt)\n",
    "# data_pos = {i: [] for i in range(1, 2)}\n",
    "# print(data_pos)\n",
    "\n",
    "# for i in range(data_gt.shape[0]):\n",
    "#     for j in range(data_gt.shape[1]):\n",
    "#         if data_gt[i, j]:\n",
    "#             data_pos[1].append([i, j])\n",
    "# data_t = 0\n",
    "# data_pos_all = list()\n",
    "\n",
    "# for k,v in data_pos.items():\n",
    "#     print('data-ID %s: %s'%(k,len(v)))\n",
    "#     data_t += len(v)\n",
    "#     for t in v:\n",
    "#         data_pos_all.append([k,t])\n",
    "# print('total data %s'%data_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 创建一个空的ndarray 用于装数据\n",
    "# data_all = np.zeros((54129, cube_size, cube_size, 204))\n",
    "# data_label_all = np.zeros((54129)).astype(\"int\")\n",
    "\n",
    "# k = 0\n",
    "# for i in data_pos_all:\n",
    "#     # print(i)\n",
    "#     # 取出训练集中的一个数，随着 i 的改变， 所取的数也会发生改变\n",
    "#     [r,c] = i[1]\n",
    "#     # print(i[1])\n",
    "#     # print(r, c)\n",
    "#     # pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32).tostring()\n",
    "#     pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32)\n",
    "#     data_all[k] = pixel_t\n",
    "#     # print(pixel_t.shape)\n",
    "#     # print(train[1000])\n",
    "    \n",
    "#     # 标签值 - 1\n",
    "#     label_t = np.array(np.array(i[0] - 1).astype(np.int64))\n",
    "#     data_label_all[k] = label_t\n",
    "#     k = k+ 1\n",
    "\n",
    "# data_all.shape, data_label_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in data_pos.items():\n",
    "#     print(k, end=\",\")\n",
    "\n",
    "# print()\n",
    "# for k, v in data_pos.items():\n",
    "#     print(len(v), end=\",\")"
   ]
  },
  {
   "source": [
    "# 划分训练集和测试集"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到类别数 16类，从1开始\n",
    "# DataSet salinas_corrected shape is (512, 217, 204)\n",
    "# 在这里已经把标签为 0 的背景给删除了\n",
    "\n",
    "class_num = np.max(data_gt)    \n",
    "\n",
    "data_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "train_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "test_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "\n",
    "print(data_pos)\n",
    "\n",
    "for i in range(data_gt.shape[0]):\n",
    "    for j in range(data_gt.shape[1]):\n",
    "        for k in range(1, class_num + 1):\n",
    "            if data_gt[i, j] == k:\n",
    "                data_pos[k].append([i, j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否用随机种子\n",
    "if fix_seed:\n",
    "    random.seed(seed)\n",
    "\n",
    "# 划分训练集和测试集, 一共 3200 个\n",
    "for k, v in data_pos.items():\n",
    "    if len(v)<train_num:\n",
    "        train_num = 15\n",
    "    else:\n",
    "        train_num = train_num\n",
    "    train_pos[k] = random.sample(v, int(train_num))\n",
    "    test_pos[k] = [i for i in v if i not in train_pos[k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_all = list()\n",
    "test_pos_all = list()\n",
    "for k,v in train_pos.items():\n",
    "    for t in v:\n",
    "        train_pos_all.append([k,t])\n",
    "for k,v in test_pos.items():\n",
    "    for t in v:\n",
    "        test_pos_all.append([k,t])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_t = 0\n",
    "test_t = 0\n",
    "for (k1,v1),(k2,v2) in zip(train_pos.items(), test_pos.items()):\n",
    "    print('traindata-ID %s: %s; testdata-ID %s: %s'%(k1,len(v1),k2,len(v2)))\n",
    "    train_t += len(v1)\n",
    "    test_t += len(v2)\n",
    "print('total train %s, total test %s'%(train_t,test_t))\n",
    "# for k,v in self.test_pos.items():\n",
    "#     print('testdata-ID %s: %s'%(k,len(v)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空的ndarray 用于装数据\n",
    "train = np.zeros((3200, cube_size, cube_size, 204)).astype(np.float32)\n",
    "train_label = np.zeros((3200)).astype(np.int32)\n",
    "\n",
    "test = np.zeros((50929, cube_size, cube_size, 204)).astype(np.float32)\n",
    "test_label = np.zeros((50929)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "k = 0\n",
    "for i in train_pos_all:\n",
    "    # print(i)\n",
    "    # 取出训练集中的一个数，随着 i 的改变， 所取的数也会发生改变\n",
    "    [r,c] = i[1]\n",
    "    # print(i[1])\n",
    "    # print(r, c)\n",
    "    # pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32).tostring()\n",
    "    pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32)\n",
    "    train[k] = pixel_t\n",
    "    # print(pixel_t.shape)\n",
    "    # print(train[1000])\n",
    "    \n",
    "    # 标签值 - 1\n",
    "    label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "    train_label[k] = label_t\n",
    "    k = k+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "k = 0\n",
    "for i in test_pos_all:\n",
    "    # 取出测试集中的一个数，随着 i 的改变， 所取的数也会发生改变\n",
    "    [r, c] = i[1]\n",
    "    # pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32).tostring()\n",
    "    pixel_t = neighbor_add(r,c,w_size=cube_size).astype(np.float32)\n",
    "    test[k] = pixel_t\n",
    "\n",
    "    label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "    test_label[k] = label_t\n",
    "    # print('.', end='')\n",
    "    k = k+ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape, test.shape,train_label.shape, test.shape"
   ]
  },
  {
   "source": [
    "# 创建dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "\n",
    "# db_train = db_train.shuffle(3800).batch(batch_size=BATCHSZ).repeat()\n",
    "db_train = db_train.shuffle(3800).batch(batch_size=BATCHSZ)    ###############\n",
    "db_test = db_test.batch(batch_size=BATCHSZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train, db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, filter_num, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # padding 对于能够整除的四周均匀补全，对于不能整除的，自适应补全\n",
    "        # 第一层\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3,3), strides=stride, padding='same')\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "\n",
    "        # 第二层\n",
    "        self.conv2 = layers.Conv2D(filter_num, (3,3), strides=1, padding='same')\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # 保证短接后的 channels 一致\n",
    "        if stride != 1:\n",
    "            # 短接层， identity layer\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n",
    "        else:\n",
    "            self.downsample = lambda x:x\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 前向传播\n",
    "        # [b, h ,w, c]\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        identity = self.downsample(inputs)\n",
    "\n",
    "        output = layers.add([out, identity])\n",
    "        output = self.relu(output)              # output = tf.nn.relu(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 是多个 BasicBlock 顿叠而成\n",
    "class ResNet(keras.Model):\n",
    "\n",
    "    def __init__(self, layer_dims, num_calsses=16):   #  layer_dims [2,2,2,2] 每一层的basic block\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # 设置预处理层\n",
    "        self.stem = Sequential([layers.Conv2D(64, (3,3), strides=(1, 1)),\n",
    "                               layers.BatchNormalization(),\n",
    "                               layers.Activation('relu'),\n",
    "                               layers.MaxPooling2D(pool_size=(2,2), strides=(1, 1), padding='same')\n",
    "                               ])\n",
    "\n",
    "        self.layers1 = self.build_resblock(64, layer_dims[0])\n",
    "        # h, w 维会变小\n",
    "        self.layers2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layers3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layers4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "\n",
    "        # 全连接层 output : [b, 512, h, w]，自适应输出用于输出\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()\n",
    "        self.fc = layers.Dense(16)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 前向运算过程，预处理\n",
    "        x = self.stem(inputs)\n",
    "        # 4 个 resBlock\n",
    "        x = self.layers1(x)\n",
    "        x = self.layers2(x)\n",
    "        x = self.layers3(x)\n",
    "        x = self.layers4(x)\n",
    "\n",
    "        # [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # [b, 100]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 实现 resblock\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):\n",
    "        res_blocks = Sequential()\n",
    "        # 添加第一层， may down sample\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        # 后面的 BasicBlock 不会下采样  ？？？# TODO？？？\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "        return res_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    return ResNet([2, 2, 2, 2])   #   1 + 4 * 4 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = next(iter(db_train))\n",
    "print(sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"res_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (None, 7, 7, 64)          117824    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (None, 7, 7, 64)          148736    \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (None, 4, 4, 128)         526976    \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (None, 2, 2, 256)         2102528   \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (None, 1, 1, 512)         8399360   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  8208      \n",
      "=================================================================\n",
      "Total params: 11,303,632\n",
      "Trainable params: 11,295,824\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n",
      "0 0 loss 2.768153190612793\n",
      "0 acc 0.39005281862985725\n",
      "1 0 loss 1.5118151903152466\n",
      "1 acc 0.482082899723144\n",
      "2 0 loss 1.0202693939208984\n",
      "2 acc 0.6284631545877595\n",
      "3 0 loss 0.821701169013977\n",
      "3 acc 0.5990496573661371\n",
      "4 0 loss 1.3626755475997925\n",
      "4 acc 0.7586640224626441\n",
      "5 0 loss 0.37392985820770264\n",
      "5 acc 0.7396571697853875\n",
      "6 0 loss 0.4052172303199768\n",
      "6 acc 0.6110467513597361\n",
      "7 0 loss 0.6878067255020142\n",
      "7 acc 0.8193563588525202\n",
      "8 0 loss 0.33610403537750244\n",
      "8 acc 0.8607669500677414\n",
      "9 0 loss 0.18082872033119202\n",
      "9 acc 0.6811443381963125\n",
      "10 0 loss 0.6193172931671143\n"
     ]
    }
   ],
   "source": [
    "model = resnet18()\n",
    "model.build(input_shape=(None, 9, 9, 204))  # TODO 需要修改\n",
    "model.summary()\n",
    "optimizer = optimizers.Adam(lr=1e-3)\n",
    "\n",
    "for epoch in range(30):\n",
    "    for step, (x, y) in enumerate(db_train):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # [b, 32, 32, 3]  =>  [b, 100]\n",
    "            logits = model(x)\n",
    "            # print(logits.shape)\n",
    "            # [b] => [b, 100]\n",
    "            y_onthot = tf.one_hot(y, depth=num_calsses)         # TODO 需要修改\n",
    "            # print(y_onthot.shape) \n",
    "            loss = tf.losses.categorical_crossentropy(y_onthot, logits, from_logits=True)\n",
    "            loss = tf.reduce_mean(loss)\n",
    "\n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if step % 300 ==0:\n",
    "            print(epoch, step, 'loss', float(loss))\n",
    "\n",
    "    total_num = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    for x, y in db_test:\n",
    "        logits = model(x)\n",
    "        prob = tf.nn.softmax(logits, axis=1)\n",
    "        pred = tf.argmax(prob, axis=1)\n",
    "        pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "        correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "        correct = tf.reduce_sum(correct)\n",
    "        \n",
    "        total_num += x.shape[0]\n",
    "        \n",
    "        total_correct += int(correct)\n",
    "\n",
    "    acc = total_correct / total_num\n",
    "    print(epoch, 'acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model'):\n",
    "    os.mkdir('model')\n",
    "model.save_weights('./model/weights.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = resnet18()\n",
    "model2.build(input_shape=(None, 9, 9, 204))\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=10, min_lr=0.000001, verbose=1)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(\n",
    "              optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 30\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to train model \n",
    "history = model2.fit(db_train,\n",
    "                    batch_size=batch_size, \n",
    "                    steps_per_epoch=train.shape[0]//batch_size,\n",
    "                    epochs=epoch, \n",
    "                    validation_data=db_test, \n",
    "                    validation_steps=test.shape[0]//batch_size,\n",
    "                    callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}