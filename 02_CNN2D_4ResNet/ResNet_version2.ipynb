{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 在上一个代码的基础上，加一些注释，修改数据获取的方式"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras import models, Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential, datasets, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "BATCHSZ = 32\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train_label.npy\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3200, 9, 9, 204), (3200,), (50929, 9, 9, 204), (50929,))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "data_dir= \"E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)\n",
    "train = np.load(data_root[4])\n",
    "train_label = np.load(data_root[5])\n",
    "test = np.load(data_root[2])\n",
    "test_label = np.load(data_root[3])\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_num = 16\n",
    "im_height = 9\n",
    "im_width = 9\n",
    "im_channel = train.shape[3]\n",
    "train_num = train.shape[0]\n",
    "val_num = test.shape[0]"
   ]
  },
  {
   "source": [
    "# 创建dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "db_train = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_num).repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "db_train = db_train.shuffle(buffer_size=train_num).batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "\n",
    "# load test dataset\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "# val_dataset = val_dataset.repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "db_test = db_test.batch(BATCHSZ).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>,\n",
       " <PrefetchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "db_train, db_test"
   ]
  },
  {
   "source": [
    "# 构建 ResNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- add（）：直接对张量求和,add层将dense_1层的输入和dense_2层的输入加在了一起，是张量元素内容相加。\n",
    "- conatenate（）：串联一个列表的输入张量。对一维进行了串联，通道数变成了x + x = 2x，可以指指定 axis = x 来指定空间的第 x 维串联。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实现残差块\n",
    "# (3,3) (1,1) 是卷积核的大小\n",
    "\n",
    "class BasicBlock(layers.Layer):\n",
    "\n",
    "    def __init__(self, filter_num, stride=1):   \n",
    "        super(BasicBlock, self).__init__()\n",
    "\n",
    "        # padding 对于能够整除的四周均匀补全，对于不能整除的，自适应补全，保证能够用到原始图像的全部像素信息，\n",
    "        # 并不是保证 padding 后图像大小一致，当 stride = 2， 使用 padding = same时，必使结果的图像大小变为一半\n",
    "        # “第一层” strides=stride，有时进行下采样（stride > 1）,有时不进行下采样\n",
    "        self.conv1 = layers.Conv2D(filter_num, (3,3), strides=stride, padding='same', use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.relu = layers.Activation('relu')\n",
    "\n",
    "        # “第二层”\n",
    "        self.conv2 = layers.Conv2D(filter_num, (3,3), strides=1, padding='same', use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        \n",
    "        # \"分支层\"\n",
    "        # 保证从 x 直接连到下面的线两端能够之间相加，如果上一个 Residual Block 的输出维度和当前的 Residual Block 的维度不一样，\n",
    "        # 那就对这个 x 进行 downSample 操作，使得维度一致\n",
    "        if stride != 1:\n",
    "            # 短接层， identity layer，这里的 strides 与第一层的 strides 相同，保证结果可以和第二层直接相加\n",
    "            self.downsample = Sequential()\n",
    "            self.downsample.add(layers.Conv2D(filter_num, (1, 1), strides=stride))\n",
    "        else:\n",
    "            # 如果 strides = 1，保证结果可以和第二层直接相加，就不需要 downsample\n",
    "            self.downsample = lambda x:x\n",
    "\n",
    "    # 残差块内正向传播过程，包含两个卷积层\n",
    "    def call(self, inputs, training=None):\n",
    "        # 前向传播\n",
    "        # [b, h ,w, c]\n",
    "        out = self.conv1(inputs)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        # 此处调用 downsample  有两种形式，看stride 的值而定\n",
    "        identity = self.downsample(inputs)\n",
    "\n",
    "        output = layers.add([out, identity])    # 这里的相加是对应元素相加\n",
    "        output = self.relu(output)              # 没有参数的层可以定义一个层用两次\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet 是多个 BasicBlock 顿叠而成\n",
    "class ResNet(keras.Model):\n",
    "    \n",
    "    # layer_dims [2,2,2,2]\n",
    "    def __init__(self, layer_dims, num_calsses=16):   # layer_dims [2,2,2,2] 每一层的basic block个数\n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        # 设置预处理层\n",
    "        self.stem = Sequential([layers.Conv2D(64, (3,3), strides=(1, 1)),\n",
    "                               layers.BatchNormalization(),\n",
    "                               layers.Activation('relu'),\n",
    "                               layers.MaxPooling2D(pool_size=(2,2), strides=(1, 1), padding='same')\n",
    "                               ])\n",
    "\n",
    "        # 中间层\n",
    "        self.layers1 = self.build_resblock(64, layer_dims[0])   # 调用数组layer_dims中的第 0 维定义的 basicBlock个数\n",
    "        \n",
    "        # h, w 维会变小，这里stride 等于 2，使得 feature size 越来越小，channel 越来越多\n",
    "        self.layers2 = self.build_resblock(128, layer_dims[1], stride=2)\n",
    "        self.layers3 = self.build_resblock(256, layer_dims[2], stride=2)\n",
    "        self.layers4 = self.build_resblock(512, layer_dims[3], stride=2)\n",
    "\n",
    "        # 分类层\n",
    "        # 全连接层 output : [b, 512, h, w]，自适应输出用于输出\n",
    "        self.avgpool = layers.GlobalAveragePooling2D()  # 具体大小为6 × 6 × 3，经过GAP转换后，变成了大小为 1 × 1 × 3 的输出值，每一层 h × w 会被平均化成一个值\n",
    "        self.fc = layers.Dense(16)  # TODO\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # 前向运算过程，预处理\n",
    "        x = self.stem(inputs)\n",
    "        # 4 个 resBlock\n",
    "        x = self.layers1(x)\n",
    "        x = self.layers2(x)\n",
    "        x = self.layers3(x)\n",
    "        x = self.layers4(x)\n",
    "\n",
    "        # [b, c]\n",
    "        x = self.avgpool(x)\n",
    "        # [b, 16]\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    # 实现 resblock\n",
    "    # 创建一个resBlock， 一个 resBlock 中包含多个 basicBlock\n",
    "    def build_resblock(self, filter_num, blocks, stride=1):    # 通道数，ResNet 会堆叠多少，默认步长为 1\n",
    "        res_blocks = Sequential()\n",
    "        # 添加第一层， may down sample\n",
    "        res_blocks.add(BasicBlock(filter_num, stride))\n",
    "        # 后面的 BasicBlock 不会下采样，因为此处定义了 stride 为 1\n",
    "        for _ in range(1, blocks):\n",
    "            res_blocks.add(BasicBlock(filter_num, stride=1))\n",
    "        return res_blocks"
   ]
  },
  {
   "source": [
    "- 传给类中的参数，都被 \\__init__ ()中的形式变量接收了\n",
    "- 因为类中已经实现，\\__call__() 魔法方法,因此不用外部调用就可以运行build()、call()等函数？"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet18():\n",
    "    return ResNet([2, 2, 2, 2])   #   1 + 4 * 4 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(32, 9, 9, 204) (32,) tf.Tensor(0.000542417, shape=(), dtype=float32) tf.Tensor(0.90160555, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(db_train))\n",
    "print(sample[0].shape, sample[1].shape, tf.reduce_min(sample[0]), tf.reduce_max(sample[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    model = resnet18()\n",
    "    model.build(input_shape=(None, 9, 9, 204))  # TODO 需要修改\n",
    "    model.summary()\n",
    "    optimizer = optimizers.Adam(lr=1e-3)\n",
    "    \n",
    "    for epoch in range(30):\n",
    "        for step, (x, y) in enumerate(db_train):\n",
    "            with tf.GradientTape() as tape:\n",
    "                # [b, 32, 32, 3]  =>  [b, 100]\n",
    "                logits = model(x)\n",
    "                # print(logits.shape)\n",
    "                # [b] => [b, 100]\n",
    "                y_onthot = tf.one_hot(y, depth=class_num)         # TODO 需要修改\n",
    "                # print(y_onthot.shape) \n",
    "                loss = tf.losses.categorical_crossentropy(y_onthot, logits, from_logits=True)\n",
    "                loss = tf.reduce_mean(loss)\n",
    "\n",
    "            grads = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            if step % 300 ==0:\n",
    "                print(epoch, step, 'loss', float(loss))\n",
    "\n",
    "        total_num = 0\n",
    "        total_correct = 0\n",
    "\n",
    "        for x, y in db_test:\n",
    "            logits = model(x)\n",
    "            prob = tf.nn.softmax(logits, axis=1)\n",
    "            pred = tf.argmax(prob, axis=1)\n",
    "            pred = tf.cast(pred, dtype=tf.int32)\n",
    "\n",
    "            correct = tf.cast(tf.equal(pred, y), dtype=tf.int32)\n",
    "            correct = tf.reduce_sum(correct)\n",
    "            \n",
    "            total_num += x.shape[0]\n",
    "            \n",
    "            total_correct += int(correct)\n",
    "\n",
    "        acc = total_correct / total_num\n",
    "        print(epoch, 'acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"res_net_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_16 (Sequential)   (None, 7, 7, 64)          117824    \n",
      "_________________________________________________________________\n",
      "sequential_17 (Sequential)   (None, 7, 7, 64)          148480    \n",
      "_________________________________________________________________\n",
      "sequential_18 (Sequential)   (None, 4, 4, 128)         526464    \n",
      "_________________________________________________________________\n",
      "sequential_20 (Sequential)   (None, 2, 2, 256)         2101504   \n",
      "_________________________________________________________________\n",
      "sequential_22 (Sequential)   (None, 1, 1, 512)         8397312   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_2 ( multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  8208      \n",
      "=================================================================\n",
      "Total params: 11,299,792\n",
      "Trainable params: 11,291,984\n",
      "Non-trainable params: 7,808\n",
      "_________________________________________________________________\n",
      "0 0 loss 2.799243211746216\n",
      "0 acc 0.44658249720198706\n",
      "1 0 loss 1.695272445678711\n",
      "1 acc 0.4806888020577667\n",
      "2 0 loss 1.5102754831314087\n",
      "2 acc 0.5518466885271652\n",
      "3 0 loss 1.2209233045578003\n",
      "3 acc 0.3957862907184512\n",
      "4 0 loss 1.030914306640625\n",
      "4 acc 0.6329596104380608\n",
      "5 0 loss 0.6626710891723633\n",
      "5 acc 0.6624320131948399\n",
      "6 0 loss 0.4527164399623871\n",
      "6 acc 0.6575035834200553\n",
      "7 0 loss 0.5917708277702332\n",
      "7 acc 0.7693651946827937\n",
      "8 0 loss 0.6263127326965332\n",
      "8 acc 0.82907577215339\n",
      "9 0 loss 0.36316511034965515\n",
      "9 acc 0.6933181487953818\n",
      "10 0 loss 0.37455159425735474\n",
      "10 acc 0.8260715898603939\n",
      "11 0 loss 0.15676812827587128\n",
      "11 acc 0.8728425847748826\n",
      "12 0 loss 0.14942783117294312\n",
      "12 acc 0.7757858980148835\n",
      "13 0 loss 0.38393712043762207\n",
      "13 acc 0.7932808419564492\n",
      "14 0 loss 0.1996849626302719\n",
      "14 acc 0.8475524750142355\n",
      "15 0 loss 0.3824231028556824\n",
      "15 acc 0.8496338039231086\n",
      "16 0 loss 0.25145891308784485\n",
      "16 acc 0.8346521628148992\n",
      "17 0 loss 0.424856036901474\n",
      "17 acc 0.8757289559975653\n",
      "18 0 loss 0.3995105028152466\n",
      "18 acc 0.8607473148893557\n",
      "19 0 loss 0.14769247174263\n",
      "19 acc 0.8679730605352549\n",
      "20 0 loss 0.15903356671333313\n",
      "20 acc 0.8808733727345913\n",
      "21 0 loss 0.11330126225948334\n",
      "21 acc 0.8098725676922774\n",
      "22 0 loss 0.31020790338516235\n",
      "22 acc 0.852284553005164\n",
      "23 0 loss 0.27971595525741577\n",
      "23 acc 0.8394431463409845\n",
      "24 0 loss 0.5649091005325317\n",
      "24 acc 0.8083999293133578\n",
      "25 0 loss 0.36446911096572876\n",
      "25 acc 0.8563490349309824\n",
      "26 0 loss 0.1604262888431549\n",
      "26 acc 0.909246205501777\n",
      "27 0 loss 0.06434286385774612\n",
      "27 acc 0.8754147931433957\n",
      "28 0 loss 0.19943495094776154\n",
      "28 acc 0.872921125488425\n",
      "29 0 loss 0.19036179780960083\n",
      "29 acc 0.8209271731233678\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}