{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Sequential\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "source": [
    "标准的ReesNet\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "# GPU 的问题！！\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\data_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\test_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\\train_label.npy\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((3200, 9, 9, 204), (3200,), (50929, 9, 9, 204), (50929,))"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data_dir= \"E:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)\n",
    "train = np.load(data_root[4])\n",
    "train_label = np.load(data_root[5])\n",
    "test = np.load(data_root[2])\n",
    "test_label = np.load(data_root[3])\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = tf.keras.utils.to_categorical(train_label)\n",
    "test_label = tf.keras.utils.to_categorical(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSZ = 32\n",
    "batch_size = 32\n",
    "epochs =10\n",
    "class_num = 16\n",
    "im_height = 9\n",
    "im_width = 9\n",
    "im_channel = train.shape[3]\n",
    "train_num = train.shape[0]\n",
    "val_num = test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据可用的CPU动态设置并行调用的数量， 应用于 num_parallel_calls\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "# load train dataset\n",
    "db_train = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "# train_dataset = train_dataset.shuffle(buffer_size=train_num).repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "train_dataset = db_train.shuffle(buffer_size=train_num).batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "\n",
    "# load test dataset\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "# val_dataset = val_dataset.repeat().batch(BATCHSZ).prefetch(AUTOTUNE)\n",
    "val_dataset = db_test.batch(BATCHSZ).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>,\n",
       " <PrefetchDataset shapes: ((None, 9, 9, 204), (None, 16)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "db_train, db_test\n",
    "train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(layers.Layer):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, out_channel, strides=1, downsample=None, **kwargs):\n",
    "        super(BasicBlock, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(out_channel, kernel_size=3, strides=strides,\n",
    "                                   padding=\"SAME\", use_bias=False)\n",
    "        self.bn1 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = layers.Conv2D(out_channel, kernel_size=3, strides=1,\n",
    "                                   padding=\"SAME\", use_bias=False)\n",
    "        self.bn2 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5)\n",
    "        # -----------------------------------------\n",
    "        self.downsample = downsample\n",
    "        self.relu = layers.ReLU()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        identity = inputs\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "\n",
    "        x = self.add([identity, x])\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(layers.Layer):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, out_channel, strides=1, downsample=None, **kwargs):\n",
    "        super(Bottleneck, self).__init__(**kwargs)\n",
    "        self.conv1 = layers.Conv2D(out_channel, kernel_size=1, use_bias=False, name=\"conv1\")\n",
    "        self.bn1 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv1/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.conv2 = layers.Conv2D(out_channel, kernel_size=3, use_bias=False,\n",
    "                                   strides=strides, padding=\"SAME\", name=\"conv2\")\n",
    "        self.bn2 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv2/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.conv3 = layers.Conv2D(out_channel * self.expansion, kernel_size=1, use_bias=False, name=\"conv3\")\n",
    "        self.bn3 = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv3/BatchNorm\")\n",
    "        # -----------------------------------------\n",
    "        self.relu = layers.ReLU()\n",
    "        self.downsample = downsample\n",
    "        self.add = layers.Add()\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        identity = inputs\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(inputs)\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.bn1(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x, training=training)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x, training=training)\n",
    "\n",
    "        x = self.add([x, identity])\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_layer(block, in_channel, channel, block_num, name, strides=1):\n",
    "    downsample = None\n",
    "    if strides != 1 or in_channel != channel * block.expansion:\n",
    "        downsample = Sequential([\n",
    "            layers.Conv2D(channel * block.expansion, kernel_size=1, strides=strides,\n",
    "                          use_bias=False, name=\"conv1\"),\n",
    "            layers.BatchNormalization(momentum=0.9, epsilon=1.001e-5, name=\"BatchNorm\")\n",
    "        ], name=\"shortcut\")\n",
    "\n",
    "    layers_list = []\n",
    "    layers_list.append(block(channel, downsample=downsample, strides=strides, name=\"unit_1\"))\n",
    "\n",
    "    for index in range(1, block_num):\n",
    "        layers_list.append(block(channel, name=\"unit_\" + str(index + 1)))\n",
    "\n",
    "    return Sequential(layers_list, name=name)\n",
    "\n",
    "\n",
    "def _resnet(block, blocks_num, im_width=224, im_height=224, num_classes=1000, include_top=True):\n",
    "    # tensorflow中的tensor通道排序是NHWC\n",
    "    # (None, 224, 224, 3)\n",
    "    # change\n",
    "    input_image = layers.Input(shape=(im_height, im_width, 204), dtype=\"float32\")\n",
    "    x = layers.Conv2D(filters=64, kernel_size=7, strides=2,\n",
    "                      padding=\"SAME\", use_bias=False, name=\"conv1\")(input_image)\n",
    "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=\"conv1/BatchNorm\")(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, padding=\"SAME\")(x)\n",
    "\n",
    "    x = _make_layer(block, x.shape[-1], 64, blocks_num[0], name=\"block1\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 128, blocks_num[1], strides=2, name=\"block2\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 256, blocks_num[2], strides=2, name=\"block3\")(x)\n",
    "    x = _make_layer(block, x.shape[-1], 512, blocks_num[3], strides=2, name=\"block4\")(x)\n",
    "\n",
    "    if include_top:\n",
    "        x = layers.GlobalAvgPool2D()(x)  # pool + flatten\n",
    "        x = layers.Dense(num_classes, name=\"logits\")(x)\n",
    "        predict = layers.Softmax()(x)\n",
    "    else:\n",
    "        predict = x\n",
    "\n",
    "    model = Model(inputs=input_image, outputs=predict)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet34(im_width=224, im_height=224, num_classes=1000, include_top=True):\n",
    "    return _resnet(BasicBlock, [3, 4, 6, 3], im_width, im_height, num_classes, include_top)\n",
    "\n",
    "\n",
    "def resnet50(im_width=224, im_height=224, num_classes=1000, include_top=True):\n",
    "    return _resnet(Bottleneck, [3, 4, 6, 3], im_width, im_height, num_classes, include_top)\n",
    "\n",
    "\n",
    "def resnet101(im_width=224, im_height=224, num_classes=1000, include_top=True):\n",
    "    return _resnet(Bottleneck, [3, 4, 23, 3], im_width, im_height, num_classes, include_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nfunctional_1 (Functional)    (None, 1, 1, 512)         21932032  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 512)               0         \n_________________________________________________________________\ndropout (Dropout)            (None, 512)               0         \n_________________________________________________________________\ndense (Dense)                (None, 1024)              525312    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 1024)              0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                16400     \n_________________________________________________________________\nsoftmax (Softmax)            (None, 16)                0         \n=================================================================\nTotal params: 22,473,744\nTrainable params: 22,456,720\nNon-trainable params: 17,024\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 实例化模型\n",
    "feature = resnet34(im_width=9, im_height=9, num_classes=class_num, include_top=False)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([feature,\n",
    "                                tf.keras.layers.GlobalAvgPool2D(),\n",
    "                                tf.keras.layers.Dropout(rate=0.5),\n",
    "                                tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
    "                                tf.keras.layers.Dropout(rate=0.5),\n",
    "                                tf.keras.layers.Dense(16),\n",
    "                                tf.keras.layers.Softmax()])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# using keras low level api for training\n",
    "loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output = model(images, training=True)\n",
    "        loss = loss_object(labels, output)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, output)\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    output = model(images, training=False)\n",
    "    t_loss = loss_object(labels, output)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6.606709599999931\n",
      "Epoch 1, Loss: 4.409997463226318, Accuracy: 9.09375, Test Loss: 2.304157018661499, Test Accuracy: 27.873584747314453\n",
      "2.6892331999999897\n",
      "Epoch 2, Loss: 3.5891764163970947, Accuracy: 15.5625, Test Loss: 1.804103970527649, Test Accuracy: 49.717159271240234\n",
      "2.6978610000001026\n",
      "Epoch 3, Loss: 3.0134928226470947, Accuracy: 21.53125, Test Loss: 1.3721567392349243, Test Accuracy: 63.16192626953125\n",
      "2.693465100000026\n",
      "Epoch 4, Loss: 2.4246490001678467, Accuracy: 31.593748092651367, Test Loss: 1.158398985862732, Test Accuracy: 61.29399490356445\n",
      "2.7028881000001093\n",
      "Epoch 5, Loss: 2.0023787021636963, Accuracy: 40.40625, Test Loss: 0.861285388469696, Test Accuracy: 69.9658203125\n",
      "2.7055960999999797\n",
      "Epoch 6, Loss: 1.6792712211608887, Accuracy: 49.71875, Test Loss: 0.6510846018791199, Test Accuracy: 76.99952697753906\n",
      "2.7018593999999894\n",
      "Epoch 7, Loss: 1.455611228942871, Accuracy: 56.3125, Test Loss: 0.5890112519264221, Test Accuracy: 77.44735717773438\n",
      "2.704505999999924\n",
      "Epoch 8, Loss: 1.250504732131958, Accuracy: 62.25, Test Loss: 0.5220466256141663, Test Accuracy: 78.30374145507812\n",
      "2.7043087999999216\n",
      "Epoch 9, Loss: 1.1203103065490723, Accuracy: 65.53125, Test Loss: 0.4799272119998932, Test Accuracy: 81.73908233642578\n",
      "2.704263900000001\n",
      "Epoch 10, Loss: 1.08235502243042, Accuracy: 67.6875, Test Loss: 0.4620087146759033, Test Accuracy: 82.5993881225586\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "train_step_num = train_num // batch_size\n",
    "val_step_num = val_num // batch_size\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss.reset_states()        # clear history info\n",
    "    train_accuracy.reset_states()    # clear history info\n",
    "    test_loss.reset_states()         # clear history info\n",
    "    test_accuracy.reset_states()     # clear history info\n",
    "\n",
    "    t1 = time.perf_counter()\n",
    "    for index, (images, labels) in enumerate(train_dataset):\n",
    "        train_step(images, labels)\n",
    "        if index+1 == train_step_num:\n",
    "            break\n",
    "    print(time.perf_counter()-t1)\n",
    "\n",
    "    for index, (images, labels) in enumerate(val_dataset):\n",
    "        test_step(images, labels)\n",
    "        if index+1 == val_step_num:\n",
    "            break\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch,\n",
    "                            train_loss.result(),\n",
    "                            train_accuracy.result() * 100,\n",
    "                            test_loss.result(),\n",
    "                            test_accuracy.result() * 100))\n",
    "    if test_loss.result() < best_test_loss:\n",
    "        model.save_weights(\"./save_weights/myResNet.ckpt\", save_format='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}