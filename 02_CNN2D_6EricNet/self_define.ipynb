{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# keras.Sequential\n",
    "- model.trainable_variables\n",
    "- model.call()\n",
    "# keras.layers.Layer\n",
    "- \\__init__\n",
    "- call\n",
    "\n",
    "# keras.Model\n",
    "- \\__init__\n",
    "- call\n",
    "- Model:compile/fit/evaluate/predict"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "- os.environ\\[\"TF_CPP_MIN_LOG_LEVEL\"]的取值有四个：0，1，2，3，分别和log的四个等级挂钩：INFO，WARNING，ERROR，FATAL（重要性由左到右递增）\n",
    "\n",
    "- 当os.environ\\[\"TF_CPP_MIN_LOG_LEVEL\"]='0'的时候，输出信息：INFO + WARNING + ERROR + FATAL\n",
    "\n",
    "- 当os.environ\\[\"TF_CPP_MIN_LOG_LEVEL\"]='1'的时候，输出信息：WARNING + ERROR + FATAL\n",
    "\n",
    "- 当os.environ\\[\"TF_CPP_MIN_LOG_LEVEL\"]='2'的时候，输出信息：ERROR + FATAL\n",
    "\n",
    "- 当os.environ\\[\"TF_CPP_MIN_LOG_LEVEL\"]='3'的时候，输出信息：FATAL"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\n",
    "from tensorflow import keras\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]='2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    # [0-255]  - [-1, 1]\n",
    "    x = 2 * tf.cast(x, dtype=tf.float32) / 255. - 1\n",
    "    y = tf.cast(y, dtype=tf.int32)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchsz = 128\n",
    "(x, y), (x_val, y_val) = datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((50000, 32, 32, 3),\n",
       " TensorShape([50000, 10]),\n",
       " (10000, 32, 32, 3),\n",
       " TensorShape([10000, 10]),\n",
       " 0,\n",
       " 255)"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# [10k, 1]  [10k]  [10k , 10]\n",
    "y = tf.squeeze(y)\n",
    "y_val = tf.squeeze(y_val)\n",
    "y = tf.one_hot(y, depth=10)\n",
    "y_val = tf.one_hot(y_val, depth=10)\n",
    "x.shape, y.shape, x_val.shape, y_val.shape, x.min(), x.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_db = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "train_db = train_db.map(preprocess).shuffle(10000).batch(batchsz)\n",
    "test_db = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "test_db = test_db.map(preprocess).batch(batchsz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(128, 32, 32, 3) (128, 10)\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_db))\n",
    "print(sample[0].shape, sample[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDense(layers.Layer):\n",
    "    # to replace standard layers.Dence()\n",
    "    def __init__(self, inp_dim, outp_dim):\n",
    "        super(MyDense, self).__init__()\n",
    "        #  Layer.add_variable is deprecated\n",
    "        # self.kernel = self.add_variable('w', [inp_dim, outp_dim])\n",
    "        # self.bias = self.add_variable('b', [outp_dim])   \n",
    "        self.kernel = self.add_weight('w', [inp_dim, outp_dim])\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs @ self.kernel\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNetwork(keras.Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(MyNetwork, self).__init__()\n",
    "\n",
    "        self.fc1 = MyDense(32*32*3, 256)\n",
    "        self.fc2 = MyDense(256, 128)\n",
    "        self.fc3 = MyDense(128, 64)\n",
    "        self.fc4 = MyDense(64, 32)\n",
    "        self.fc5 = MyDense(32, 10)\n",
    "\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        # forward\n",
    "        \"\"\"\n",
    "        inputs: [b, 32, 32, 3]\n",
    "        training\n",
    "\n",
    "        \"\"\"\n",
    "        x = tf.reshape(inputs, [-1, 32*32*3])\n",
    "\n",
    "        # (b, 32*32*3)   [b, 256]\n",
    "        x = self.fc1(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # [b, 256]  [b, 128]\n",
    "        x = self.fc2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # [b, 128]  [b, 64]\n",
    "        x = self.fc3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # [b, 64]  [b, 32]\n",
    "        x = self.fc4(x)\n",
    "        x = tf.nn.relu(x)\n",
    "\n",
    "        # [b, 32]  [b, 10]\n",
    "        x = self.fc5(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "netWork = MyNetwork()\n",
    "netWork.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.7305 - acc: 0.3886 - val_loss: 1.5806 - val_acc: 0.4407\n",
      "Epoch 2/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.5115 - acc: 0.4692 - val_loss: 1.5136 - val_acc: 0.4609\n",
      "Epoch 3/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.4109 - acc: 0.5010 - val_loss: 1.4362 - val_acc: 0.4976\n",
      "Epoch 4/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.3271 - acc: 0.5322 - val_loss: 1.4184 - val_acc: 0.5039\n",
      "Epoch 5/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2628 - acc: 0.5563 - val_loss: 1.4213 - val_acc: 0.5068\n",
      "Epoch 6/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1947 - acc: 0.5794 - val_loss: 1.4190 - val_acc: 0.5082\n",
      "Epoch 7/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.1387 - acc: 0.5995 - val_loss: 1.4074 - val_acc: 0.5229\n",
      "Epoch 8/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0800 - acc: 0.6199 - val_loss: 1.4456 - val_acc: 0.5178\n",
      "Epoch 9/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.0273 - acc: 0.6379 - val_loss: 1.4122 - val_acc: 0.5208\n",
      "Epoch 10/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9784 - acc: 0.6576 - val_loss: 1.4738 - val_acc: 0.5229\n",
      "Epoch 11/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.9250 - acc: 0.6752 - val_loss: 1.4762 - val_acc: 0.5284\n",
      "Epoch 12/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8750 - acc: 0.6910 - val_loss: 1.5095 - val_acc: 0.5260\n",
      "Epoch 13/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.8320 - acc: 0.7076 - val_loss: 1.5859 - val_acc: 0.5213\n",
      "Epoch 14/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7899 - acc: 0.7209 - val_loss: 1.5960 - val_acc: 0.5223\n",
      "Epoch 15/15\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.7494 - acc: 0.7336 - val_loss: 1.6604 - val_acc: 0.5206\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x115be086ee0>"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "netWork.fit(train_db, epochs=15, validation_data=test_db, validation_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "79/79 [==============================] - 0s 3ms/step - loss: 1.6604 - acc: 0.5206\n",
      "saved to ckpt/weights.ckpt\n"
     ]
    }
   ],
   "source": [
    "netWork.evaluate(test_db)\n",
    "if not os.path.exists('ckpt'):\n",
    "    os.mkdir('ckpt')\n",
    "netWork.save_weights('ckpt/weights.ckpt')\n",
    "print(\"saved to ckpt/weights.ckpt\")\n",
    "del netWork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "load ckpt/weights.ckpt\n",
      "79/79 [==============================] - 0s 3ms/step - loss: 1.6604 - acc: 0.5206\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[1.6604353189468384, 0.5206000208854675]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "netWork = MyNetwork()\n",
    "netWork.compile(optimizer=optimizers.Adam(lr=1e-3),\n",
    "                loss=tf.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                metrics=['acc'])\n",
    "netWork.load_weights('ckpt/weights.ckpt')\n",
    "print(\"load ckpt/weights.ckpt\")\n",
    "netWork.evaluate(test_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}