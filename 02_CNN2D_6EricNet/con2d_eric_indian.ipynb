{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, BatchNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "\n",
    "from model import net\n",
    "%matplotlib inline\n",
    "#from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSZ = 32\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['E:\\\\Eric_HSI\\\\hyper_data_preprocess\\\\Indian_pines_w_size_9_num_200_for_2D\\\\data.npy', 'E:\\\\Eric_HSI\\\\hyper_data_preprocess\\\\Indian_pines_w_size_9_num_200_for_2D\\\\data_label.npy', 'E:\\\\Eric_HSI\\\\hyper_data_preprocess\\\\Indian_pines_w_size_9_num_200_for_2D\\\\test.npy', 'E:\\\\Eric_HSI\\\\hyper_data_preprocess\\\\Indian_pines_w_size_9_num_200_for_2D\\\\test_label.npy', 'E:\\\\Eric_HSI\\\\hyper_data_preprocess\\\\Indian_pines_w_size_9_num_200_for_2D\\\\train.npy', 'E:\\\\Eric_HSI\\\\hyper_data_preprocess\\\\Indian_pines_w_size_9_num_200_for_2D\\\\train_label.npy']\n"
     ]
    }
   ],
   "source": [
    "data_dir= \"E:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "E:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\\data.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\\data_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\\test.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\\test_label.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\\train.npy\nE:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_9_num_200_for_2D\\train_label.npy\n"
     ]
    }
   ],
   "source": [
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2460, 9, 9, 200), (2460,), (7789, 9, 9, 200), (7789,))"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "train = np.load(data_root[4])\n",
    "train_label = np.load(data_root[5])\n",
    "test = np.load(data_root[2])\n",
    "test_label = np.load(data_root[3])\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2460, 9, 9, 200), (2460, 16), (7789, 9, 9, 200), (7789, 16))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# Reshape data into (numberofsumples, channels, height, width)\n",
    "\n",
    "# convert class labels to on-hot encoding\n",
    "train_label = utils.to_categorical(train_label)\n",
    "test_label = utils.to_categorical(test_label)\n",
    "train.shape, train_label.shape, test.shape, test_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(dtype('float32'), dtype('float32'), dtype('float32'), dtype('float32'))"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "train.dtype, test.dtype, train_label.dtype, test_label.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "\n",
    "db_train = db_train.shuffle(train.shape[0]).repeat().batch(batch_size=BATCHSZ)\n",
    "db_test = db_test.batch(batch_size=BATCHSZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 9, 9, 200), (None, 16)), types: (tf.float32, tf.float32)>,\n",
       " <BatchDataset shapes: ((None, 9, 9, 200), (None, 16)), types: (tf.float32, tf.float32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "db_train, db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"functional_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 9, 9, 200)]       0         \n_________________________________________________________________\nnet_block (NetBlock)         (None, 31, 31, 256)       1324876   \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 256)               0         \n_________________________________________________________________\ndense (Dense)                (None, 256)               65792     \n_________________________________________________________________\ndropout (Dropout)            (None, 256)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 512)               131584    \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 512)               0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 16)                8208      \n=================================================================\nTotal params: 1,530,460\nTrainable params: 1,527,980\nNon-trainable params: 2,480\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = net(im_width=9, im_height=9, im_channel=200 ,num_classes=16, include_top=True)\n",
    "model.build(input_shape=(None, 9, 9, 204))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('model_indian'):\n",
    "    os.mkdir('model_indian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization and train method\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=10, min_lr=0.000001, verbose=1)\n",
    "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath='./model_indian/indian.h5',\n",
    "                                                save_best_only=True,\n",
    "                                                # save_weights_only=True,\n",
    "                                                monitor='val_loss')]\n",
    "# checkpointer = ModelCheckpoint(filepath=\".\\checkP\\.checkpoint.ckpt\", verbose=1, save_best_only=False)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(\n",
    "              optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCHSZ = 32\n",
    "EPOCHS = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to train model \n",
    "history = model.fit(db_train,\n",
    "                    batch_size=BATCHSZ, \n",
    "                    steps_per_epoch=train.shape[0]//BATCHSZ,\n",
    "                    epochs=EPOCHS, \n",
    "                    validation_data=db_test, \n",
    "                    validation_steps=test.shape[0]//BATCHSZ,\n",
    "                    callbacks=[reduce_lr, callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}