{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "source": [
    "# 定义一些常量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 9   # 创建的数据立方体大小\n",
    "batch_size = 32\n",
    "train_num = 200\n",
    "seed = 666\n",
    "fix_seed = False\n",
    "savedata = True\n",
    "\n",
    "name1 = \"paviaU.mat\"\n",
    "name1_gt = \"paviaU_gt.mat\"\n",
    "\n",
    "name2 = \"Salinas_corrected.mat\"\n",
    "name2_gt = \"Salinas_gt.mat\""
   ]
  },
  {
   "source": [
    "# 定义数据处理所需要的函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def max_min(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    data_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Salinas_corrected.mat\")\n",
    "    data_gt_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Salinas_gt.mat\")\n",
    "    # startswith 检查字符串是否以 \"————\" 开头, 取出数据集\n",
    "    data_name = [t for t in list(data_dict.keys()) if not t.startswith('__')][0]\n",
    "    data_gt_name = [t for t in list(data_gt_dict.keys()) if not t.startswith('__')][0]\n",
    "    data = data_dict[data_name]\n",
    "    data_gt = data_gt_dict[data_gt_name].astype(np.int32)\n",
    "    # 标准化\n",
    "    data = max_min(data).astype(np.float32)\n",
    "    class_num = np.max(data_gt)\n",
    "    print('DataSet %s shape is %s class_num is %s'%(data_name,data.shape,class_num))\n",
    "    return data, data_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, data_gt = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否用随机种子\n",
    "if fix_seed:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分类,将各类别放到以各类别名为键的字典中,这里只保存的坐标位置，并不是像素值\n",
    "\n",
    "def split_background(removeZeroLabels=False):\n",
    "    # 这个是未分类版本\n",
    "    class_num = np.max(data_gt)\n",
    "    data_pos = {i: [] for i in range(0, 1)}\n",
    "    print(data_pos)\n",
    "\n",
    "    for i in range(data_gt.shape[0]):\n",
    "        for j in range(data_gt.shape[1]):\n",
    "            if removeZeroLabels:\n",
    "                if data_gt[i, j]:\n",
    "                    data_pos[0].append([i, j])\n",
    "            else:\n",
    "                data_pos[0].append([i, j])\n",
    "\n",
    "    return data_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pos = split_background(removeZeroLabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pos = split_background(removeZeroLabels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集,将各类别放到以各类别名为键的训练集和测试集字典中，这里只保存的坐标位置，并不是像素值\n",
    "# 在这里已经把标签为 0 的背景给删除了\n",
    "def split_train_test(train_num=200):\n",
    "    class_num = np.max(data_gt)\n",
    "\n",
    "    data_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "    print(data_pos)\n",
    "\n",
    "    train_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "    test_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "\n",
    "    for i in range(data_gt.shape[0]):\n",
    "        for j in range(data_gt.shape[1]):\n",
    "            for k in range(1, class_num + 1):\n",
    "                if data_gt[i, j] == k:\n",
    "                    data_pos[k].append([i, j])\n",
    "\n",
    "    for k, v in data_pos.items():\n",
    "        if len(v)<train_num:\n",
    "            train_seclect = 15\n",
    "        else:\n",
    "            train_seclect = train_num\n",
    "        train_pos[k] = random.sample(v, int(train_seclect))\n",
    "        test_pos[k] = [i for i in v if i not in train_pos[k]]\n",
    "    return train_pos, test_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos, test_pos = split_train_test(train_num=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 字典转列表,但是这时依然是字典中的信息\n",
    "def dict_to_list():\n",
    "    data_pos_all = list()\n",
    "    train_pos_all = list()\n",
    "    test_pos_all = list()\n",
    "\n",
    "    for k,v in data_pos.items():\n",
    "        for t in v:\n",
    "            data_pos_all.append([k,t])\n",
    "\n",
    "    for k,v in train_pos.items():\n",
    "        for t in v:\n",
    "            train_pos_all.append([k,t])\n",
    "\n",
    "    for k,v in test_pos.items():\n",
    "        for t in v:\n",
    "            test_pos_all.append([k,t])\n",
    "            \n",
    "    return data_pos_all, train_pos_all, test_pos_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pos_all, train_pos_all, test_pos_all = dict_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数，查看各类别信息\n",
    "def classinfo(train_pos, test_pos, data_pos):\n",
    "    data_num = 0\n",
    "    train_num = 0\n",
    "    test_num = 0\n",
    "    train_test_num = 0\n",
    "    for (k1,v1),(k2,v2) in zip(train_pos.items(), test_pos.items()):\n",
    "        print('traindata-ID %s: %s; testdata-ID %s: %s'%(k1,len(v1),k2,len(v2)))\n",
    "        train_num += len(v1)\n",
    "        test_num += len(v2)\n",
    "    train_test_num = train_num + test_num\n",
    "    print('total train %s, total test %s, train_test_num %s'%(train_num, test_num, train_test_num))\n",
    "\n",
    "    for k,v in data_pos.items():\n",
    "        data_num += len(v)\n",
    "    print('total data %s'%data_num)\n",
    "\n",
    "    return data_num, train_num, test_num, train_test_num "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_num, train_num, test_num, train_test_num  = classinfo(train_pos, test_pos, data_pos)"
   ]
  },
  {
   "source": [
    "# 第一次调用一部分函数：到创建 patch 之前"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataSet salinas_corrected shape is (512, 217, 204) class_num is 16\n",
      "{0: []}\n",
      "{1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: [], 10: [], 11: [], 12: [], 13: [], 14: [], 15: [], 16: []}\n",
      "traindata-ID 1: 200; testdata-ID 1: 1809\n",
      "traindata-ID 2: 200; testdata-ID 2: 3526\n",
      "traindata-ID 3: 200; testdata-ID 3: 1776\n",
      "traindata-ID 4: 200; testdata-ID 4: 1194\n",
      "traindata-ID 5: 200; testdata-ID 5: 2478\n",
      "traindata-ID 6: 200; testdata-ID 6: 3759\n",
      "traindata-ID 7: 200; testdata-ID 7: 3379\n",
      "traindata-ID 8: 200; testdata-ID 8: 11071\n",
      "traindata-ID 9: 200; testdata-ID 9: 6003\n",
      "traindata-ID 10: 200; testdata-ID 10: 3078\n",
      "traindata-ID 11: 200; testdata-ID 11: 868\n",
      "traindata-ID 12: 200; testdata-ID 12: 1727\n",
      "traindata-ID 13: 200; testdata-ID 13: 716\n",
      "traindata-ID 14: 200; testdata-ID 14: 870\n",
      "traindata-ID 15: 200; testdata-ID 15: 7068\n",
      "traindata-ID 16: 200; testdata-ID 16: 1607\n",
      "total train 3200, total test 50929, train_test_num 54129\n",
      "total data 111104\n"
     ]
    }
   ],
   "source": [
    "data, data_gt = loadData()\n",
    "data_pos = split_background(removeZeroLabels=False)\n",
    "train_pos, test_pos = split_train_test(train_num=200)\n",
    "data_pos_all, train_pos_all, test_pos_all = dict_to_list()\n",
    "data_num, train_num, test_num, train_test_num  = classinfo(train_pos, test_pos, data_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个空的ndarray 用于装数据\n",
    "data_all = np.zeros((data_num, window_size, window_size, data.shape[2])).astype(np.float32)\n",
    "data_label_all = np.zeros((data_num)).astype(np.int32)\n",
    "\n",
    "train_all = np.zeros((train_num, window_size, window_size, data.shape[2])).astype(np.float32)\n",
    "train_label_all = np.zeros((train_num)).astype(np.int32)\n",
    "\n",
    "test_all = np.zeros((test_num, window_size, window_size, data.shape[2])).astype(np.float32)\n",
    "test_label_all = np.zeros((test_num)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all.shape, data_label_all.shape, train_all.shape, train_label_all.shape, test_all.shape, test_label_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_patch = neighbor_add\n",
    "def neighbor_add(row, col, window_size=3):  \n",
    "    t = window_size // 2\n",
    "    # 初始化立方体 shape = 1, 1, 204\n",
    "    cube = np.zeros(shape=[window_size, window_size, data.shape[2]])\n",
    "    for i in range(-t, t + 1):\n",
    "        for j in range(-t, t + 1):\n",
    "            # 如果创建的 cube 在图像之外\n",
    "            if i + row < 0 or i + row >= data.shape[0] or j + col < 0 or j + col >= data.shape[1]:\n",
    "                cube[i + t, j + t] = data[row, col]\n",
    "            else:\n",
    "                cube[i + t, j + t] = data[i + row, j + col]\n",
    "    return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cube_t = cube_target\n",
    "def create_data_all():\n",
    "    k = 0\n",
    "    for i in data_pos_all:\n",
    "        [r,c] = i[1]\n",
    "        cube_t = neighbor_add(r,c,window_size=window_size).astype(np.float32)\n",
    "        data_all[k] = cube_t\n",
    "        # 标签值 - 1\n",
    "        label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "        data_label_all[k] = label_t\n",
    "        k = k + 1\n",
    "    return data_all, data_label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cube_t = cube_target\n",
    "def create_train_all():\n",
    "    k = 0\n",
    "    for i in train_pos_all:\n",
    "        [r,c] = i[1]\n",
    "        cube_t = neighbor_add(r,c,window_size=window_size).astype(np.float32)\n",
    "        train_all[k] = cube_t\n",
    "        # 标签值 - 1\n",
    "        label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "        train_label_all[k] = label_t\n",
    "        k = k + 1\n",
    "    return train_all, train_label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cube_t = cube_target\n",
    "def create_test_all():\n",
    "    k = 0\n",
    "    for i in test_pos_all:\n",
    "        [r,c] = i[1]\n",
    "        cube_t = neighbor_add(r,c,window_size=window_size).astype(np.float32)\n",
    "        test_all[k] = cube_t\n",
    "        # 标签值 - 1\n",
    "        label_t = np.array(np.array(i[0] - 1).astype(np.int32))\n",
    "        test_label_all[k] = label_t\n",
    "        k = k + 1\n",
    "    return test_all, test_label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all, data_label_all = create_data_all()\n",
    "# train_all, train_label_all = create_train_all()\n",
    "# test_all, test_label_all = create_test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all.shape, data_label_all.shape, train_all.shape, train_label_all.shape, test_all.shape, test_label_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePreprocessedData(path, data, data_label, train, train_label, test, test_label):\n",
    "    data_path = os.path.join(os.getcwd(), path)\n",
    "    print(data_path)\n",
    "\n",
    "    if savedata:\n",
    "        with open(os.path.join(data_path, 'data.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_all)\n",
    "        with open(os.path.join(data_path, 'data_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_label_all)  \n",
    "\n",
    "        with open(os.path.join(data_path, 'train.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train_all)\n",
    "        with open(os.path.join(data_path, 'train_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train_label_all)\n",
    "        \n",
    "        with open(os.path.join(data_path, 'test.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test_all)\n",
    "        with open(os.path.join(data_path, 'test_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test_label_all)"
   ]
  },
  {
   "source": [
    "# 第二次调用这些函数：数据预处理结束并保存数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all, data_label_all = create_data_all()\n",
    "train_all, train_label_all = create_train_all()\n",
    "test_all, test_label_all = create_test_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((111104, 9, 9, 204),\n",
       " (111104,),\n",
       " (3200, 9, 9, 204),\n",
       " (3200,),\n",
       " (50929, 9, 9, 204),\n",
       " (50929,))"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "data_all.shape, data_label_all.shape, train_all.shape, train_label_all.shape, test_all.shape, test_label_all.shape"
   ]
  },
  {
   "source": [
    "if not os.path.exists('Salinas_w_size_'+ str(window_size) + '_num_200_for_2D'):\n",
    "    os.mkdir('Salinas_w_size_'+ str(window_size) + '_num_200_for_2D')\n",
    "savePreprocessedData('Salinas_w_size_'+ str(window_size) + '_num_200_for_2D', data_all, data_label_all, train_all, train_label_all, test_all, test_label_all)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:\\Eric_HSI\\hyper_data_preprocess\\Salinas_w_size_9_num_200_for_2D\n"
     ]
    }
   ]
  },
  {
   "source": [
    "# 创建datase"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建datase\n",
    "db_train = tf.data.Dataset.from_tensor_slices((train_all, train_label_all))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test_all, test_label_all))\n",
    "\n",
    "# 自定义训练函数不用 repeat\n",
    "db_train = db_train.shuffle(train_num).batch(batch_size=batch_size)\n",
    "db_test = db_test.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>,\n",
       " <BatchDataset shapes: ((None, 9, 9, 204), (None,)), types: (tf.float32, tf.int32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "db_train, db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}