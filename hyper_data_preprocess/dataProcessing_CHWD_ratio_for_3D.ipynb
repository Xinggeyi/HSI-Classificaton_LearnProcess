{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# 构造用于三维卷积神经网络的数据集\n",
    "- 用周围19个像素的值取预测一个值，貌似太大了\n",
    "- 将得到的数据集储存起来，让卷积模块调用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "source": [
    "# 定义常量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 19   # 创建的数据立方体大小\n",
    "batch_size = 32\n",
    "train_num = 200\n",
    "seed = 666\n",
    "savedata = True"
   ]
  },
  {
   "source": [
    "# 导入数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def max_min(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadData():\n",
    "    data_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Indian_pines_corrected.mat\")\n",
    "    data_gt_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Indian_pines_gt.mat\")\n",
    "    # startswith 检查字符串是否以 \"————\" 开头, 取出数据集\n",
    "    data_name = [t for t in list(data_dict.keys()) if not t.startswith('__')][0]\n",
    "    data_gt_name = [t for t in list(data_gt_dict.keys()) if not t.startswith('__')][0]\n",
    "    data = data_dict[data_name]\n",
    "    data_gt = data_gt_dict[data_gt_name].astype(np.int32)\n",
    "    # 标准化\n",
    "    data = max_min(data).astype(np.float32)\n",
    "    class_num = np.max(data_gt)\n",
    "    print('DataSet %s shape is %s class_num is %s'%(data_name,data.shape,class_num))\n",
    "    return data, data_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, data_gt = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_add():\n",
    "    # t array([-9, -8, -7, -6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "    t = np.array([i - window_size // 2 for i in range(window_size)])\n",
    "\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            index = i * data.shape[1] + j\n",
    "            for p in range(window_size):\n",
    "                for q in range(window_size):\n",
    "                    # [p][q] 表示对小平面进行遍历\n",
    "                    # % 表示取余\n",
    "                    # 对所创建的 n 个小立方体进行填充\n",
    "                    # 这里对于不能取满的，取对称的对角元素的值，明显不太好呀\n",
    "                    # print(\"i,j\",i, j)\n",
    "                    num_1 = (i + t[p]) % data.shape[0]\n",
    "                    # print(\"p,q\", p, q)\n",
    "                    num_2 = (j + t[q]) % data.shape[1]\n",
    "                    # print(\"num_1, num_2\",num_1, num_2)\n",
    "                    data_all[index][p][q] = data[num_1][num_2]\n",
    "    return data_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data_all = neighbor_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 去除背景，标签减一\n",
    "def split_background():\n",
    "    global data_all\n",
    "    \n",
    "    data_label_all = np.reshape(data_gt, (-1, 1))\n",
    "    tmp = np.unique(data_label_all)\n",
    "    index = np.where(np.reshape(data_label_all, (-1)) == 0)   # 返回坐标tuple(ndarray)\n",
    "    data_label_all = np.delete(data_label_all, index, axis=0)\n",
    "    \n",
    "    # 因为本来中标签没用0，但是经过Len(labels)，np.where 定位索引位置，又从0开始，所以标签值-1\n",
    "    for i in range(len(data_label_all)):\n",
    "        data_label_all[i][0] = np.where(tmp == data_label_all[i][0])[0][0]\n",
    "    data_label_all = np.reshape(data_label_all, (-1))  # 降维\n",
    "    \n",
    "    data_all = np.delete(data_all, index, axis=0)\n",
    "    return data_all, data_label_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all, data_label_all = split_background()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "def split_train_test(test_size=0.8, random_state=seed):\n",
    "    train, test, train_label, test_label = train_test_split(data_all, data_label_all, test_size=test_size, random_state=seed)\n",
    "    return train, test, train_label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePreprocessedData(path, data, data_label, train, train_label, test, test_label):\n",
    "    data_path = os.path.join(os.getcwd(), path)\n",
    "    print(data_path)\n",
    "\n",
    "    if savedata:\n",
    "        with open(os.path.join(data_path, 'data.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_all)\n",
    "        with open(os.path.join(data_path, 'data_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_label_all)  \n",
    "\n",
    "        with open(os.path.join(data_path, 'train.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train)\n",
    "        with open(os.path.join(data_path, 'train_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train_label)\n",
    "        \n",
    "        with open(os.path.join(data_path, 'test.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test)\n",
    "        with open(os.path.join(data_path, 'test_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test_label)"
   ]
  },
  {
   "source": [
    "# 调用函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataSet data shape is (145, 145, 200) class_num is 16\n"
     ]
    }
   ],
   "source": [
    "# 这个在前面定义\n",
    "data, data_gt = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(21025, 19, 19, 200)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# 创建一个 (21025, 19, 19, 200) 的数组装数据\n",
    "data_all = np.zeros((np.reshape(data_gt, (-1, 1)).shape[0], window_size, window_size, data.shape[2]), dtype='float32')\n",
    "data_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = neighbor_add()\n",
    "data_all, data_label_all = split_background()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dataset_shape:  (10249, 200, 19, 19)\ndataset_shape:  (10249, 200, 19, 19, 1)\n"
     ]
    }
   ],
   "source": [
    "data_all = data_all.swapaxes(1, 3)\n",
    "print('dataset_shape: ', data_all.shape)\n",
    "\n",
    "data_all = data_all[:, :, :, :, np.newaxis]\n",
    "print('dataset_shape: ', data_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, train_label, test_label = split_train_test(test_size=0.8, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2049, 200, 19, 19, 1), (8200, 200, 19, 19, 1), (2049,), (8200,))"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "train.shape, test.shape, train_label.shape, test_label.shape"
   ]
  },
  {
   "source": [
    "# 保存数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_19_num_0.2\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('Indian_pines_w_size_19_num_0.2_for_3D'):\n",
    "    os.mkdir('Indian_pines_w_size_19_num_0.2_for_3D')\n",
    "savePreprocessedData('Indian_pines_w_size_19_num_0.2_for_3D', data_all, data_label_all, train, train_label, test, test_label)"
   ]
  },
  {
   "source": [
    "# 创建dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_train = tf.data.Dataset.from_tensor_slices((train, train_label))\n",
    "db_test = tf.data.Dataset.from_tensor_slices((test, test_label))\n",
    "\n",
    "# 自定义训练函数不用 repeat\n",
    "db_train = db_train.shuffle(train_num).batch(batch_size=batch_size)\n",
    "db_test = db_test.batch(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<BatchDataset shapes: ((None, 200, 19, 19, 1), (None,)), types: (tf.float32, tf.int32)>,\n",
       " <BatchDataset shapes: ((None, 200, 19, 19, 1), (None,)), types: (tf.float32, tf.int32)>)"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "db_train, db_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}