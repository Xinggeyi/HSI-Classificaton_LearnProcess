{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dataset.ipynb: preprocess the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "# from skimage.transform import rotate\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "numComponents = 30\n",
    "window_size = 5\n",
    "testRatio = 0.8\n",
    "savedata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def max_min(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load the Indian pines dataset which is the .mat format\n",
    "def loadData():\n",
    "    data_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Indian_pines_corrected.mat\")\n",
    "    data_gt_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\Indian_pines_gt.mat\")\n",
    "    # startswith 检查字符串是否以 \"————\" 开头, 取出数据集\n",
    "    data_name = [t for t in list(data_dict.keys()) if not t.startswith('__')][0]\n",
    "    data_gt_name = [t for t in list(data_gt_dict.keys()) if not t.startswith('__')][0]\n",
    "    data = data_dict[data_name]\n",
    "    data_gt = data_gt_dict[data_gt_name].astype(np.int32)\n",
    "    # 标准化\n",
    "    data = max_min(data).astype(np.float32)\n",
    "    class_num = np.max(data_gt)\n",
    "    print('DataSet %s shape is %s class_num is %s'%(data_name,data.shape,class_num))\n",
    "    return data, data_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pad zeros to dataset\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset : X.shape[0] + x_offset, y_offset : X.shape[1] + y_offset, :] = X\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create Patches for dataset 顾名思义，将一整张图片划分成一个个小的patch。TotalPatNum，width，height，channels。 删除背景后，总共的 patch (10249, 5, 5, 30)  (10249,)\n",
    "# Patches windows size\n",
    "# window_size = 5\n",
    "# X (145, 145, 30)\n",
    "\n",
    "def createPatches(X, y, window_size=5, removeZeroLabels = True):\n",
    "    margin = int((window_size - 1) / 2)                                         # =>2\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)                              # (149, 149, 30)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], window_size, \n",
    "                            window_size, X.shape[2]))                            # (21025, 5, 5, 30)\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))                         # (21025,)\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "            \n",
    "    # 删除像素值为零的值\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0, : , : , :]                     # (21025, 5, 5, 30) -> (10249, 5, 5, 30)\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]                         # (10249,)\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  split data to Train and Test Set\n",
    "# X_train, Y_train, X_label, Y_label\n",
    "# (200, 5, 5, 30) (10049, 5, 5, 30) (200,) (10049,)\n",
    "\n",
    "def splitTrainTestSet(X, y, testRatio):\n",
    "#     train, test, train_label, test_label = train_test_split(X, y, \n",
    "#                                test_size=testRatio, random_state=345, stratify=y)\n",
    "    class_num = np.max(y).astype(np.int32)\n",
    "    print(class_num)\n",
    "    ss=StratifiedShuffleSplit(n_splits=class_num, test_size=testRatio, random_state=0)\n",
    "    \n",
    "    for train_index, test_index in ss.split(X, y):\n",
    "        # print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        train, test = X[train_index], X[test_index]\n",
    "        train_label, test_label = y[train_index], y[test_index]\n",
    "        \n",
    "    return train, test, train_label, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  over sample \n",
    "# 过采样，将比例比较少的例子重复几次，然后叠加到原始数据上\n",
    "# X_train:  ((200, 5, 5, 30), y_train : ((200,)\n",
    "# 变为\n",
    "# X_train : ((730, 5, 5, 30), (730,))\n",
    "\n",
    "def oversampleWeakClasses(X, y):\n",
    "    uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n",
    "    maxCount = np.max(labelCounts)\n",
    "    labelInverseRatios = maxCount / labelCounts  \n",
    "    # repeat for every label and concat\n",
    "    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), \n",
    "                                                   axis=0)\n",
    "    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        newX = np.concatenate((newX, cX))\n",
    "        newY = np.concatenate((newY, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(newY.shape[0])\n",
    "    newX = newX[rand_perm, :, :, :]\n",
    "    newY = newY[rand_perm]\n",
    "    return newX, newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##  standartize, 不要这样的标准化##\n",
    "# def standartizeData(X):\n",
    "#     newX = np.reshape(X, (-1, X.shape[3]))\n",
    "#     scaler = preprocessing.StandardScaler().fit(newX)  \n",
    "#     newX = scaler.transform(newX)\n",
    "#     newX = np.reshape(newX, (X.shape[0], X.shape[1], X.shape[2], X.shape[3]))\n",
    "#     return newX, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Augment Data\n",
    "def AugmentData(X_train):\n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "\n",
    "        if (num == 0):\n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            flipped_patch = np.fliplr(patch)\n",
    "\n",
    "        if (num == 2):\n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, \n",
    "                            no,axes=(1, 0), reshape=False, output=None, \n",
    "                            order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "        \n",
    "    patch2 = flipped_patch\n",
    "    X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePreprocessedData(path, data, data_label, train, train_label, test, test_label):\n",
    "    data_path = os.path.join(os.getcwd(), path)\n",
    "    print(data_path)\n",
    "\n",
    "    if savedata:\n",
    "        with open(os.path.join(data_path, 'data.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_all)\n",
    "        with open(os.path.join(data_path, 'data_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_label_all)  \n",
    "\n",
    "        with open(os.path.join(data_path, ' data_remove_0.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_remove_0)\n",
    "        with open(os.path.join(data_path, 'data_label_remove_0.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_label_remove_0) \n",
    "\n",
    "        with open(os.path.join(data_path, 'train.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train)\n",
    "        with open(os.path.join(data_path, 'train_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train_label)\n",
    "        \n",
    "        with open(os.path.join(data_path, 'test.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test)\n",
    "        with open(os.path.join(data_path, 'test_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test_label)"
   ]
  },
  {
   "source": [
    "# 调用函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataSet data shape is (145, 145, 200) class_num is 16\n",
      "(145, 145, 30)\n"
     ]
    }
   ],
   "source": [
    "X, y = loadData()\n",
    "X, pca = applyPCA(X, numComponents=numComponents)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "data_all (21025, 5, 5, 30), data_label_all (21025,)\n",
      "16\n",
      "(4205, 5, 5, 30) (16820, 5, 5, 30) (4205,) (16820,)\n",
      "train (36635, 5, 5, 30), train_label (36635,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "data_all, data_label_all = createPatches(X, y, window_size=window_size, removeZeroLabels = False) \n",
    "data_remove_0, data_label_remove_0 = createPatches(X, y, window_size=window_size, removeZeroLabels = True) \n",
    "print(\"data_all %s, data_label_all %s\"%(data_all.shape, data_label_all.shape))\n",
    "train, test, train_label, test_label = splitTrainTestSet(data_all, data_label_all, testRatio)\n",
    "print(train.shape, test.shape, train_label.shape, test_label.shape)\n",
    "\n",
    "train, train_label = oversampleWeakClasses(train, train_label)\n",
    "print(\"train %s, train_label %s\"%(train.shape, train_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(36635, 5, 5, 30)\n"
     ]
    }
   ],
   "source": [
    "train = AugmentData(train)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:\\Eric_HSI\\hyper_data_preprocess\\Indian_pines_w_size_5_num_0.2_for_2D\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('Indian_pines_w_size_' + str(window_size) + '_num_0.2_for_2D'):\n",
    "    os.mkdir('Indian_pines_w_size_'+ str(window_size)+'_num_0.2_for_2D')\n",
    "savePreprocessedData('Indian_pines_w_size_'+ str(window_size) +'_num_0.2_for_2D', data_all, data_label_all, train, train_label, test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}