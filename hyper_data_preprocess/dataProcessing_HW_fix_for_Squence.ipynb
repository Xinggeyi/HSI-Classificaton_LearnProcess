{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、导入数据加数据处理阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_num = 200\n",
    "seed = 666\n",
    "fix_seed = False\n",
    "savedata = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 是否用随机种子\n",
    "if fix_seed:\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据标准化\n",
    "def max_min(x):\n",
    "    return (x-np.min(x))/(np.max(x)-np.min(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入数据\n",
    "def loadData():\n",
    "    data_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\paviaU.mat\")\n",
    "    data_gt_dict = sio.loadmat(r\"E:\\Eric_HSI\\hyperspectral_datasets\\paviaU_gt.mat\")\n",
    "    # startswith 检查字符串是否以 \"————\" 开头, 取出数据集\n",
    "    data_name = [t for t in list(data_dict.keys()) if not t.startswith('__')][0]\n",
    "    data_gt_name = [t for t in list(data_gt_dict.keys()) if not t.startswith('__')][0]\n",
    "    data = data_dict[data_name]\n",
    "    data_gt = data_gt_dict[data_gt_name].astype(np.int32)\n",
    "    # 标准化\n",
    "    data = max_min(data).astype(np.float32)\n",
    "    class_num = np.max(data_gt)\n",
    "    print('DataSet %s shape is %s class_num is %s'%(data_name,data.shape,class_num))\n",
    "    return data, data_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, data_gt = loadData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据分类,将各类别放到以各类别名为键的字典中,这里只保存的坐标位置，并不是像素值\n",
    "\n",
    "def split_background(removeZeroLabels=False):\n",
    "    # 这个是未分类版本\n",
    "    class_num = np.max(data_gt)\n",
    "    data_pos = {i: [] for i in range(0, 1)}\n",
    "    print(data_pos)\n",
    "\n",
    "    for i in range(data_gt.shape[0]):\n",
    "        for j in range(data_gt.shape[1]):\n",
    "            if removeZeroLabels:\n",
    "                if data_gt[i, j]:\n",
    "                    data_pos[0].append([i, j])\n",
    "            else:\n",
    "                data_pos[0].append([i, j])\n",
    "\n",
    "    return data_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_pos = split_background(removeZeroLabels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 取出测试集和训练集,此过程后训练集和测试集为字典，和上面构造data_pos类似\n",
    "\n",
    "def split_train_test(train_num=200):\n",
    "    class_num = np.max(data_gt)\n",
    "\n",
    "    data_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "    print(data_pos)\n",
    "\n",
    "    train_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "    test_pos = {i: [] for i in range(1, class_num + 1)}\n",
    "\n",
    "    for i in range(data_gt.shape[0]):\n",
    "        for j in range(data_gt.shape[1]):\n",
    "            for k in range(1, class_num + 1):\n",
    "                if data_gt[i, j] == k:\n",
    "                    data_pos[k].append([i, j])\n",
    "\n",
    "    for k, v in data_pos.items():\n",
    "        if len(v)<train_num:\n",
    "            train_seclect = 15\n",
    "        else:\n",
    "            train_seclect = train_num\n",
    "        train_pos[k] = random.sample(v, int(train_seclect))\n",
    "        test_pos[k] = [i for i in v if i not in train_pos[k]]\n",
    "    return train_pos, test_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pos, test_pos = split_train_test(train_num=train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 辅助函数，查看各类别信息\n",
    "def classinfo(data_pos, train_pos, test_pos):\n",
    "    data_num = 0\n",
    "    train_num = 0\n",
    "    test_num = 0\n",
    "    train_test_num = 0\n",
    "    for (k1,v1),(k2,v2) in zip(train_pos.items(), test_pos.items()):\n",
    "        print('traindata-ID %s: %s; testdata-ID %s: %s'%(k1,len(v1),k2,len(v2)))\n",
    "        train_num += len(v1)\n",
    "        test_num += len(v2)\n",
    "    train_test_num = train_num + test_num\n",
    "    print('total train %s, total test %s, train_test_num %s'%(train_num, test_num, train_test_num))\n",
    "\n",
    "    for k,v in data_pos.items():\n",
    "        data_num += len(v)\n",
    "    print('total data %s'%data_num)\n",
    "\n",
    "    return data_num, train_num, test_num, train_test_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_num, train_num, test_num, train_test_num = classinfo(data_pos, train_pos, test_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将位置信息转化光谱值信息，即将坐标值转化为其对应的光谱值，此过程得到一个为列表\n",
    "# 准确的说是列表内嵌套 ndarray 的结构，ndarray有 1 维，个数103\n",
    "def dict_to_list():\n",
    "    data_all = []\n",
    "    data_label_all = []\n",
    "    train = []\n",
    "    train_label = []\n",
    "    test = []\n",
    "    test_label = []\n",
    "    for i in range(len(data_pos)):\n",
    "        for j in range(len(data_pos[i])):\n",
    "            row,col = data_pos[i][j]\n",
    "            data_all.append(data[row,col])\n",
    "            data_label_all.append(i)\n",
    "\n",
    "\n",
    "    for i in range(1,len(train_pos)+1):   # 9个类\n",
    "        for j in range(len(train_pos[i])):   # 200个样本\n",
    "            row,col = train_pos[i][j]\n",
    "            train.append(data[row,col])   #### 一下子传入103维 ####\n",
    "            train_label.append(i)\n",
    "            \n",
    "    for i in range(1,len(test_pos)+1):\n",
    "        for j in range(len(test_pos[i])):\n",
    "            row,col = test_pos[i][j]\n",
    "            test.append(data[row,col])\n",
    "            test_label.append(i)\n",
    "    return data_all, data_label_all, train, train_label, test, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_all, data_label_all, train, train_label, test, test_label = dict_to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def savePreprocessedData(path, data, data_label, train, train_label, test, test_label):\n",
    "    data_path = os.path.join(os.getcwd(), path)\n",
    "    print(data_path)\n",
    "\n",
    "    if savedata:\n",
    "        with open(os.path.join(data_path, 'data.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_all)\n",
    "        with open(os.path.join(data_path, 'data_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, data_label_all)  \n",
    "\n",
    "        with open(os.path.join(data_path, 'train.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train)\n",
    "        with open(os.path.join(data_path, 'train_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, train_label)\n",
    "        \n",
    "        with open(os.path.join(data_path, 'test.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test)\n",
    "        with open(os.path.join(data_path, 'test_label.npy'), 'bw') as outfile:\n",
    "            np.save(outfile, test_label)"
   ]
  },
  {
   "source": [
    "# 调用以上函数"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DataSet data shape is (610, 340, 103) class_num is 9\n{0: []}\n{1: [], 2: [], 3: [], 4: [], 5: [], 6: [], 7: [], 8: [], 9: []}\n"
     ]
    }
   ],
   "source": [
    "data, data_gt = loadData()\n",
    "data_pos = split_background(removeZeroLabels=False)\n",
    "train_pos, test_pos = split_train_test(train_num=train_num)\n",
    "data_all, data_label_all, train, train_label, test, test_label = dict_to_list()\n",
    "# data_num, train_num, test_num, train_test_num = classinfo(data_pos, train_pos, test_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### np.array & np.asarray\n",
    "- array和asarray都可以将结构数据转化为ndarray，\n",
    "- 但是主要区别就是当数据源是ndarray时，array仍然会copy出一个副本，占用新的内存，但asarray不会。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last step: 化为ndarray!\n",
    "data_all = np.asarray(data_all)\n",
    "data_label_all = np.asarray(data_label_all)\n",
    "train = np.asarray(train)\n",
    "train_label = np.asarray(train_label)\n",
    "test = np.asarray(test)\n",
    "test_label = np.asarray(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((207400, 103), (207400,), (1800, 103), (40976, 103), (1800,), (40976,))"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "data_all.shape, data_label_all.shape, train.shape, test.shape, train_label.shape, test_label.shape"
   ]
  },
  {
   "source": [
    "# 保存数据"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "e:\\Eric_HSI\\hyper_data_preprocess\\paviaU_num_200_for_squence\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('paviaU_num_'+ str(train_num) +'_for_squence'):\n",
    "    os.mkdir('paviaU_num_'+ str(train_num) +'_for_squence')\n",
    "savePreprocessedData('paviaU_num_'+ str(train_num) +'_for_squence', data_all, data_label_all, train, train_label, test, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}