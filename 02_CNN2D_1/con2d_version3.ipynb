{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.ipynb: Define and Train the model\n",
    "- 测试各种读取数据的方式\n",
    "- 读取数据升级\n",
    "- 修改反向传播的训练方法为Adam\n",
    "- 加入model.compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import numpy as np\n",
    "import scipy\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import utils\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Variables\n",
    "# The number of principal components to be retained in the PCA algorithm, \n",
    "# the number of retained features  n\n",
    "numPCAcomponents = 30\n",
    "# Patches windows size\n",
    "windowSize = 5\n",
    "# The proportion of Test sets\n",
    "testRatio = 0.50"
   ]
  },
  {
   "source": [
    "## 用glob 库进行读取"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['./predata\\\\readme.txt', './predata\\\\X_All_WS_5_PCA_30_testRatio_0.8.npy', './predata\\\\X_test_WS_5_PCA_30_testRatio_0.8.npy', './predata\\\\X_train_WS_5_PCA_30_testRatio_0.8.npy', './predata\\\\y_All_WS_5_PCA_30_testRatio_0.8.npy', './predata\\\\y_test_WS_5_PCA_30_testRatio_0.8.npy', './predata\\\\y_train_WS_5_PCA_30_testRatio_0.8.npy']\n"
     ]
    }
   ],
   "source": [
    "data_dir= \"./predata\"\n",
    "data_root = glob.glob(data_dir + '/*')\n",
    "print(data_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "./predata\\readme.txt\n./predata\\X_All_WS_5_PCA_30_testRatio_0.8.npy\n./predata\\X_test_WS_5_PCA_30_testRatio_0.8.npy\n./predata\\X_train_WS_5_PCA_30_testRatio_0.8.npy\n./predata\\y_All_WS_5_PCA_30_testRatio_0.8.npy\n./predata\\y_test_WS_5_PCA_30_testRatio_0.8.npy\n./predata\\y_train_WS_5_PCA_30_testRatio_0.8.npy\n"
     ]
    }
   ],
   "source": [
    "for name in glob.glob(data_dir + '/*'):\n",
    "    print(name)"
   ]
  },
  {
   "source": [
    "## 用pathlib读取"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "predata\\readme.txt\npredata\\X_All_WS_5_PCA_30_testRatio_0.8.npy\npredata\\X_test_WS_5_PCA_30_testRatio_0.8.npy\npredata\\X_train_WS_5_PCA_30_testRatio_0.8.npy\npredata\\y_All_WS_5_PCA_30_testRatio_0.8.npy\npredata\\y_test_WS_5_PCA_30_testRatio_0.8.npy\npredata\\y_train_WS_5_PCA_30_testRatio_0.8.npy\n"
     ]
    }
   ],
   "source": [
    "data_dir= \"./predata\"\n",
    "data_root = pathlib.Path(data_dir)\n",
    "for item in data_root.iterdir():\n",
    "    print(item)"
   ]
  },
  {
   "source": [
    "## 用OS读取"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['readme.txt',\n",
       " 'X_All_WS_5_PCA_30_testRatio_0.8.npy',\n",
       " 'X_test_WS_5_PCA_30_testRatio_0.8.npy',\n",
       " 'X_train_WS_5_PCA_30_testRatio_0.8.npy',\n",
       " 'y_All_WS_5_PCA_30_testRatio_0.8.npy',\n",
       " 'y_test_WS_5_PCA_30_testRatio_0.8.npy',\n",
       " 'y_train_WS_5_PCA_30_testRatio_0.8.npy']"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "# os.listdir 该函数返回指定的文件夹包含的文件或文件夹的名字的列表。\n",
    "data_dir= \"./predata\"\n",
    "data_root = os.listdir(path=data_dir)\n",
    "data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      ".\\predata\\readme.txt\n.\\predata\\X_All_WS_5_PCA_30_testRatio_0.8.npy\n.\\predata\\X_test_WS_5_PCA_30_testRatio_0.8.npy\n.\\predata\\X_train_WS_5_PCA_30_testRatio_0.8.npy\n.\\predata\\y_All_WS_5_PCA_30_testRatio_0.8.npy\n.\\predata\\y_test_WS_5_PCA_30_testRatio_0.8.npy\n.\\predata\\y_train_WS_5_PCA_30_testRatio_0.8.npy\n"
     ]
    }
   ],
   "source": [
    "# 应用\n",
    "data_dir= \".\\predata\"\n",
    "for each_file in os.listdir(data_dir):\n",
    "    print(os.path.join(data_dir,each_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportion of Test sets\n",
    "testRatio = 0.80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((7919, 5, 5, 30), (7919,), (8200, 5, 5, 30), (8200,))"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "# 则是一种导入特定数据的方法，没有毛病\n",
    "X_train = np.load(\"./predata/X_train_WS_\" \n",
    "                  + str(windowSize) + \"_PCA_\" + str(numPCAcomponents) + \n",
    "                  \"_testRatio_\" + str(testRatio)  + \".npy\")\n",
    "y_train = np.load(\"./predata/y_train_WS_\" \n",
    "                  + str(windowSize) + \"_PCA_\" + str(numPCAcomponents) + \n",
    "                  \"_testRatio_\" + str(testRatio) + \".npy\")\n",
    "X_test = np.load(\"./predata/X_test_WS_\" \n",
    "                  + str(windowSize) + \"_PCA_\" + str(numPCAcomponents) + \n",
    "                 \"_testRatio_\" + str(testRatio)  + \".npy\")\n",
    "y_test = np.load(\"./predata/y_test_WS_\" \n",
    "                  + str(windowSize) + \"_PCA_\" + str(numPCAcomponents) + \n",
    "                 \"_testRatio_\" + str(testRatio) + \".npy\")\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((7919, 5, 5, 30), (7919,), (8200, 5, 5, 30), (8200,))"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(dtype('float64'), dtype('float64'), dtype('float64'), dtype('float64'))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X_train.dtype, X_test.dtype, y_train.dtype, y_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(5, 5, 30)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "90"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "# Reshape data into (numberofsumples, channels, height, width)\n",
    "\n",
    "# convert class labels to on-hot encoding\n",
    "y_train = utils.to_categorical(y_train)\n",
    "y_test = utils.to_categorical(y_test)\n",
    "\n",
    "# Define the input shape \n",
    "input_shape= X_train[0].shape\n",
    "print(input_shape)\n",
    "\n",
    "# number of filters\n",
    "C1 = 3*numPCAcomponents\n",
    "C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((7919, 5, 5, 30), (7919, 16), (8200, 5, 5, 30), (8200, 16))"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model structure\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(C1, (3, 3), activation='relu', input_shape=input_shape))\n",
    "model.add(Conv2D(3*C1, (3, 3), activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(6*numPCAcomponents, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(16, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nconv2d (Conv2D)              (None, 3, 3, 90)          24390     \n_________________________________________________________________\nconv2d_1 (Conv2D)            (None, 1, 1, 270)         218970    \n_________________________________________________________________\ndropout (Dropout)            (None, 1, 1, 270)         0         \n_________________________________________________________________\nflatten (Flatten)            (None, 270)               0         \n_________________________________________________________________\ndense (Dense)                (None, 180)               48780     \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 180)               0         \n_________________________________________________________________\ndense_1 (Dense)              (None, 16)                2896      \n=================================================================\nTotal params: 295,036\nTrainable params: 295,036\nNon-trainable params: 0\n_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimization and train method\n",
    "\n",
    "# Reduce learning rate when a metric has stopped improving.\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9, patience=25, \n",
    "                              min_lr=0.000001, verbose=1)\n",
    "# checkpointer = ModelCheckpoint(filepath=\".\\checkP\\.checkpoint.h5\", verbose=1, \n",
    "#                                save_best_only=False)\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "# sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer=adam, \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/5\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.6709 - accuracy: 0.7809 - val_loss: 0.5385 - val_accuracy: 0.8143\n",
      "Epoch 2/5\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.1496 - accuracy: 0.9519 - val_loss: 0.4023 - val_accuracy: 0.8596\n",
      "Epoch 3/5\n",
      "248/248 [==============================] - 1s 3ms/step - loss: 0.0716 - accuracy: 0.9778 - val_loss: 0.3860 - val_accuracy: 0.8867\n",
      "Epoch 4/5\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.0501 - accuracy: 0.9836 - val_loss: 0.4773 - val_accuracy: 0.8695\n",
      "Epoch 5/5\n",
      "248/248 [==============================] - 1s 2ms/step - loss: 0.0495 - accuracy: 0.9848 - val_loss: 0.4921 - val_accuracy: 0.8707\n"
     ]
    }
   ],
   "source": [
    "# Start to train model \n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=32, \n",
    "                    epochs=5, \n",
    "                    verbose=1, \n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[reduce_lr],\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model with h5py\n",
    "# import h5py\n",
    "# from tensorflow.keras.models import load_model\n",
    "# model.save('./model/HSI_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\ndict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy', 'lr'])\n"
     ]
    }
   ],
   "source": [
    "# using plot_model module to save the model figure\n",
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(model, to_file='./model/model.png', show_shapes=True)\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the model figure\n",
    "model_img = plt.imread('./model/model.png')\n",
    "# plt.figure(dpi=180)\n",
    "plt.imshow(model_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(True)\n",
    "plt.legend(['train', 'test'], loc='upper left') \n",
    "plt.savefig(\"./result/model_accuracy_100.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss \n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.grid(True)\n",
    "plt.legend(['train', 'test'], loc='upper right') \n",
    "plt.savefig(\"./result/model_loss_100.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.6 64-bit ('tensorflow2': conda)",
   "metadata": {
    "interpreter": {
     "hash": "9ecbc95d31df1b277e5ec566a835865c3a9a253da4266404bdbc6ee7e31406f4"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}