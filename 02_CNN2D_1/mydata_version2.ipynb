{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "tensorflow",
   "display_name": "tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split, StratifiedShuffleSplit\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "# from skimage.transform import rotate\n",
    "import scipy.ndimage\n",
    "from spectral import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'predata'\n",
    "# Global Variables\n",
    "# The number of principal components to be retained in the PCA algorithm, \n",
    "# the number of retained features  n\n",
    "numComponents = 30\n",
    "# Patches windows size\n",
    "windowSize = 5\n",
    "# The proportion of Test sets\n",
    "testRatio = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  load the Indian pines dataset which is the .mat format\n",
    "def loadIndianPinesData():\n",
    "    data_path = 'E:\\Eric_HSI\\hyperspectral_datasets'\n",
    "    data = sio.loadmat(os.path.join(data_path, \n",
    "                      'Indian_pines_corrected.mat'))['data']\n",
    "    labels = sio.loadmat(os.path.join(data_path, \n",
    "                        'Indian_pines_gt.mat'))['groundT']\n",
    "    \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data, labels = loadIndianPinesData()\n",
    "# data.shape, labels.shape  # ((145, 145, 200), (145, 145))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  apply PCA preprocessing for data sets\n",
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    print(newX.shape)\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newX, pca = applyPCA(data, numComponents=30)"
   ]
  },
  {
   "source": [
    "# newX.shape   # (145, 145, 30)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  pad zeros to dataset, 此行代码的作用是在图像四周加了两行的0，原因是因为下面create patches的过程中，会调用一个像素四周的两行，24个像素值，在图像四周补零，会更加充分的利用仅有的像素信息，增加像素正确分类的几率。margin = 2 = (windowSize-1) /2\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2 * margin, \n",
    "                     X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + \n",
    "         y_offset, :] = X\n",
    "    return newX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create Patches for dataset\n",
    "def createPatches(X, y, windowSize=5, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)  # （149,149,30）\n",
    "    # split patches, 创建一个想要的形状大小的矩阵即（像素总数，patches大小，patches大小，channels），（像素总数）\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patchIndex = 0\n",
    "    \n",
    "    # 从整幅图像中，将每一个像素结合边上的5*5的像素依次分别取出来\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newX, labels = createPatches(newX, labels, windowSize=5, removeZeroLabels = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newX.shape, labels.shape   # ((10249, 5, 5, 30), (10249,))"
   ]
  },
  {
   "source": [
    "- sk-learn中提StratifiedShuffleSplit()提供分层抽样功能，确保每个标签对应的样本的比例\n",
    "- 参数说明\n",
    "    - n_splits：是将训练数据分成train/test对的组数，可根据需要进行设置，默认为10\n",
    "    - test_size和train_size： 是用来设置train/test对中train和test所占的比例。例如：\n",
    "        1. 提供10个数据num进行训练和测试集划分\n",
    "        2. 设置train_size=0.8 test_size=0.2\n",
    "        3. train_num=numtrain_size=8 test_num=numtest_size=2\n",
    "     - random_state：随机数种子，和random中的seed种子一样，保证每次抽样到的数据一样，便于调试"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to Train and Test Set，将数据集划分成15组train and test，并且保证每个test中都有相同的类别\n",
    "# StratifiedShuffleSplit：在划分训练集和测试集的过程中，分成多组后，保证每个划分的数据集的标签比例相同\n",
    "def splitTrainTestSet(X, y, classnum=15, testRatio=0.50):\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "#                                test_size=testRatio, random_state=345, stratify=y)\n",
    "    ss=StratifiedShuffleSplit(n_splits=classnum, test_size=testRatio, \n",
    "                              train_size=1-testRatio, random_state=0)   # 分成15组\n",
    "    \n",
    "    for train_index, test_index in ss.split(X, y):\n",
    "        print(\"TRAIN:\", len(train_index), \"TEST:\", len(test_index))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = splitTrainTestSet(newX, labels, classnum=15, testRatio=0.50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, X_test.shape, y_train.shape, y_test.shape  # ((5124, 5, 5, 30), (5125, 5, 5, 30), (5124,), (5125,))"
   ]
  },
  {
   "source": [
    "## 过采样和欠采样都来源于信号处理\n",
    "- 在信号处理中\n",
    "1. 采样频率高于信号最高频率的两倍，这种采样被称为过采样。\n",
    "2. 采样频率低于信号最高频率的两倍，这种采样被称为欠采样。\n",
    "- 在图像分类中\n",
    "\n",
    "- 在分类问题中，有存在正反例数目差异较大的情况，这种情况叫做类别不平衡。\n",
    "- 针对这种问题，解决方式主要有3种：假设正例数量大，反例数目极小。\n",
    "    1. 减少正例的数量，使得数据平衡，再进一步分类，这种情况属于“欠采样”；\n",
    "    2. 增加反例的数目平衡数据，再分类，这种称为“过采样”；\n",
    "    3. 阈值移动方法，借用一个公式加入决策过程，提高调解不平衡性"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# over sample\n",
    "# np.unique(y, return_counts=True)：返回去重后的数组y和y中每一个元素出现的次数\n",
    "def oversampleWeakClasses(X, y):\n",
    "    uniqueLabels, labelCounts = np.unique(y, return_counts=True)  # 得到不重复的标签，标签总数ndarray\n",
    "    print(\"uniqueLabels {} \\nlabelCounts {}\".format(uniqueLabels, labelCounts))\n",
    "    maxCount = np.max(labelCounts)\n",
    "    labelInverseRatios = maxCount / labelCounts    # 应该是不平衡性的度量指标\n",
    "    print(\"labelInverseRatios\", labelInverseRatios)\n",
    "    \n",
    "    # repeat for every label and concat\n",
    "    # X(5124, 5, 5, 30) y(5124)\n",
    "    # 这两行貌似没有什么卵用，是用来验证这种方法可行的，将标签为0的重复了round(labelInverseRatios[0]次\n",
    "    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    print(newX.shape, newY.shape)\n",
    "\n",
    "    # 下面是对上面验证好的代码的使用，使用循环将每个数目较少标签都进行过采样\n",
    "    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "        cX = X[y == label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        # 每重复一次，将他们连接在原始中\n",
    "        newX = np.concatenate((newX, cX))\n",
    "        newY = np.concatenate((newY, cY))\n",
    "    \n",
    "    np.random.seed(seed=42)\n",
    "    # 随机排列序列，因为每次加上的都是相通的便签，噪声相通便签的大量重复出现，这里随机打乱顺序\n",
    "    rand_perm = np.random.permutation(newY.shape[0])\n",
    "    print(rand_perm.shape)\n",
    "    # 保证标签和数据匹配\n",
    "    newX = newX[rand_perm, :, :, :]\n",
    "    newY = newY[rand_perm]\n",
    "    return newX, newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newX, newY = oversampleWeakClasses(X_train, y_train)  \n",
    "# newX.shape, newY.shape  # ((19788, 5, 5, 30), (19788,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'numpy.ndarray'>\n[ True False False False False False False False False False]\n[0]\n[0]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# 这里验证用True 和 False 也可以作为索引\n",
    "# X[y == uniqueLabels[0], :, :, :].repeat((labelInverseRatios[0]), axis=0)\n",
    "XX = np.arange(10)\n",
    "zz = np.arange(15)\n",
    "yy = np.arange(10)\n",
    "print(type(XX))\n",
    "print(yy == zz[0])\n",
    "aa = [True, False, False, False, False, False, False, False, False, False]\n",
    "print(XX[aa])\n",
    "print(XX[yy == zz[0]])\n",
    "XX = XX[yy == zz[0]].repeat(20)\n",
    "XX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Augment Data\n",
    "def AugmentData(X_train):\n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, \n",
    "                            no,axes=(1, 0), reshape=False, output=None, \n",
    "                            order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "        \n",
    "    patch2 = flipped_patch\n",
    "    X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newX = AugmentData(newX)  \n",
    "# newX.shape    # (19788, 5, 5, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  standartize\n",
    "def standartizeData(X):\n",
    "    newX = np.reshape(X, (-1, X.shape[0]))\n",
    "    scaler = preprocessing.StandardScaler().fit(newX)  \n",
    "    newX = scaler.transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2],X.shape[3]))\n",
    "    return newX, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newX = standartizeData(newX)\n",
    "# X_test = standartizeData(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Preprocessed Data to file\n",
    "def savePreprocessedData(path, X_trainPatches, X_testPatches, y_trainPatches, \n",
    "                         y_testPatches, windowSize, wasPCAapplied = False, \n",
    "                         numPCAComponents = 0, testRatio = 0.25):\n",
    "    \n",
    "    data_path = os.path.join(os.getcwd(), path)\n",
    "    if wasPCAapplied:\n",
    "        with open(os.path.join(data_path, \"X_train_WS_\") + str(windowSize) + \n",
    "                  \"_PCA_\" + str(numPCAComponents) + \"_testRatio_\" + str(testRatio) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, X_trainPatches)\n",
    "        with open(os.path.join(data_path, \"X_test_WS_\") + str(windowSize) + \n",
    "                  \"_PCA_\" + str(numPCAComponents) + \"_testRatio_\" + str(testRatio) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, X_testPatches)\n",
    "        with open(os.path.join(data_path, \"y_train_WS_\") + str(windowSize) + \n",
    "                  \"_PCA_\" + str(numPCAComponents) + \"_testRatio_\" + str(testRatio) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, y_trainPatches)\n",
    "        with open(os.path.join(data_path, \"y_test_WS_\") + str(windowSize) + \n",
    "                  \"_PCA_\" + str(numPCAComponents) + \"_testRatio_\" + str(testRatio) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, y_testPatches)\n",
    "    else:\n",
    "        with open(os.path.join(data_path, \"preXtrainWindowSize\") + str(windowSize) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, X_trainPatches)\n",
    "        with open(os.path.join(data_path, \"preXtestWindowSize\") + str(windowSize) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, X_testPatches)\n",
    "        with open(os.path.join(data_path, \"preytrainWindowSize\") + str(windowSize) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, y_trainPatches)\n",
    "        with open(os.path.join(data_path, \"preytestWindowSize\") + str(windowSize) + \n",
    "                  \".npy\", 'bw') as outfile:\n",
    "            np.save(outfile, y_testPatches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'predata'\n",
    "# Global Variables\n",
    "# The number of principal components to be retained in the PCA algorithm, \n",
    "# the number of retained features  n\n",
    "numComponents = 30\n",
    "# Patches windows size\n",
    "windowSize = 5\n",
    "# The proportion of Test sets\n",
    "testRatio = 0.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# savePreprocessedData(path, newX, newY, X_test, y_test, windowSize = windowSize,\n",
    "#                      wasPCAapplied=True, numPCAComponents = numComponents, testRatio = testRatio)"
   ]
  },
  {
   "source": [
    "# 调用上面的函数！"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X.shape (145, 145, 200)\n",
      "y.shape (145, 145)\n",
      "(21025, 200)\n",
      "X.shape (145, 145, 30)\n"
     ]
    }
   ],
   "source": [
    "# Load dataset from file and apply PCA\n",
    "# X, y = loadHSIData()\n",
    "X, y = loadIndianPinesData()\n",
    "print('X.shape',X.shape)\n",
    "print('y.shape',y.shape)\n",
    "X, pca = applyPCA(X, numComponents=numComponents)\n",
    "print('X.shape',X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "XPatches (10249, 5, 5, 30)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "XPatches, yPatches = createPatches(X, y, windowSize=windowSize)\n",
    "print('XPatches',XPatches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "TRAIN: 5124 TEST: 5125\n",
      "X_train.shape,y_train.shape ((5124, 5, 5, 30), (5124,))\n",
      "uniqueLabels [ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15.] \n",
      "labelCounts [  23  714  415  119  241  365   14  239   10  486 1227  296  103  632\n",
      "  193   47]\n",
      "labelInverseRatios [ 53.34782609   1.71848739   2.95662651  10.31092437   5.09128631\n",
      "   3.36164384  87.64285714   5.13389121 122.7          2.52469136\n",
      "   1.           4.14527027  11.91262136   1.9414557    6.35751295\n",
      "  26.10638298]\n",
      "(1219, 5, 5, 30) (1219,)\n",
      "(19788,)\n",
      "X_train.shape,y_train.shape ((19788, 5, 5, 30), (19788,))\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = splitTrainTestSet(XPatches, yPatches, y.max()-y.min(), testRatio)\n",
    "print('X_train.shape,y_train.shape',(X_train.shape,y_train.shape))\n",
    "X_train, y_train = oversampleWeakClasses(X_train, y_train)\n",
    "print('X_train.shape,y_train.shape',(X_train.shape,y_train.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "X_train (19788, 5, 5, 30)\n"
     ]
    }
   ],
   "source": [
    "X_train = AugmentData(X_train)\n",
    "print('X_train',X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save Preprocessed Data to file\n",
    "savePreprocessedData('predata', X_train, X_test, y_train, y_test, \n",
    "                     windowSize = windowSize, wasPCAapplied=True,\n",
    "                     numPCAComponents = numComponents, testRatio = testRatio)"
   ]
  }
 ]
}